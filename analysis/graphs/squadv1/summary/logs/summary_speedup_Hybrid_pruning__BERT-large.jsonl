{"speedup": 0.9221729963255725, "f1": 91.0266636723574, "meta": {"annotate": "24", "cat_fun_name": "is_new_xp", "checkpoint": {"config": {"_name_or_path": "/home/lagunas/devel/hf/nn_pruning/nn_pruning/analysis/tmp_finetune", "architectures": ["BertForQuestionAnswering"], "attention_probs_dropout_prob": 0.1, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-12, "max_position_embeddings": 512, "model_type": "bert", "num_attention_heads": 16, "num_hidden_layers": 24, "pad_token_id": 0, "position_embedding_type": "absolute", "pruned_heads": {"0": [4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 7, 8, 10, 12, 13], "11": [0, 1, 2, 4, 5, 8, 10], "12": [2, 3, 5, 6, 7, 8, 10, 13], "13": [10, 2, 3, 12], "14": [1, 2, 3, 4, 8, 11], "15": [0, 5, 6, 7, 11, 12], "16": [3, 6, 8, 10, 13, 15], "17": [0, 2, 11, 15], "18": [2, 3, 5, 9, 11, 12, 13], "19": [0, 2, 3, 4, 9, 10, 11, 13, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15], "23": [1, 2, 4, 5, 6, 7, 9, 10, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 10, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14], "6": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 8, 9, 12, 13, 15]}, "type_vocab_size": 2, "vocab_size": 30522}, "eval_metrics": {"exact_match": 84.63576158940397, "f1": 91.0266636723574}, "path": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25_v3_f91.03/checkpoint-55000", "source_checkpoint": "/data_2to/devel_data/nn_pruning/output/squad_test_large/large_regu_10_d0.25/checkpoint-210000", "sparse_args": {"ampere_pruning_method": "disabled", "attention_block_cols": 1, "attention_block_rows": 1, "attention_lambda": 1.0, "attention_output_with_dense": 0, "attention_pruning_method": "topK", "bias_mask": true, "dense_block_cols": 1, "dense_block_rows": 1, "dense_lambda": 1.0, "dense_pruning_method": "topK", "distil_alpha_ce": 0.1, "distil_alpha_teacher": 0.9, "distil_teacher_name_or_path": "bert-large-uncased-whole-word-masking-finetuned-squad", "distil_temperature": 2.0, "final_ampere_temperature": 20.0, "final_finetune": 1, "final_threshold": 0.5, "final_warmup": 0, "initial_ampere_temperature": 0.0, "initial_threshold": 1.0, "initial_warmup": 0, "mask_init": "constant", "mask_scale": 0.0, "mask_scores_learning_rate": 0.01, "regularization": "", "regularization_final_lambda": 0}, "speed": {"cuda_eval_elapsed_time": 41.85157574462891, "eval_elapsed_time": 49.32021534908563}, "speedup": 0.9221729963255725, "stats": {"layers": {"0": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 835584, "linear_dense_total": 8388608, "linear_nnz": 2408448, "linear_total": 12582912, "nnz": 2416152, "total": 12594304}, "1": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1275904, "linear_dense_total": 8388608, "linear_nnz": 1800192, "linear_total": 12582912, "nnz": 1807343, "total": 12593536}, "10": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 2410496, "linear_dense_total": 8388608, "linear_nnz": 5031936, "linear_total": 12582912, "nnz": 5041177, "total": 12595072}, "11": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 2510848, "linear_dense_total": 8388608, "linear_nnz": 4870144, "linear_total": 12582912, "nnz": 4879242, "total": 12594880}, "12": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 2660352, "linear_dense_total": 8388608, "linear_nnz": 4757504, "linear_total": 12582912, "nnz": 4766483, "total": 12594688}, "13": {"linear_attention_nnz": 3145728, "linear_attention_total": 4194304, "linear_dense_nnz": 2605056, "linear_dense_total": 8388608, "linear_nnz": 5750784, "linear_total": 12582912, "nnz": 5760504, "total": 12595456}, "14": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 2299904, "linear_dense_total": 8388608, "linear_nnz": 4921344, "linear_total": 12582912, "nnz": 4930531, "total": 12595072}, "15": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 1699840, "linear_dense_total": 8388608, "linear_nnz": 4321280, "linear_total": 12582912, "nnz": 4330174, "total": 12595072}, "16": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 1402880, "linear_dense_total": 8388608, "linear_nnz": 4024320, "linear_total": 12582912, "nnz": 4033069, "total": 12595072}, "17": {"linear_attention_nnz": 3145728, "linear_attention_total": 4194304, "linear_dense_nnz": 1097728, "linear_dense_total": 8388608, "linear_nnz": 4243456, "linear_total": 12582912, "nnz": 4252440, "total": 12595456}, "18": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 901120, "linear_dense_total": 8388608, "linear_nnz": 3260416, "linear_total": 12582912, "nnz": 3268728, "total": 12594880}, "19": {"linear_attention_nnz": 1835008, "linear_attention_total": 4194304, "linear_dense_nnz": 739328, "linear_dense_total": 8388608, "linear_nnz": 2574336, "linear_total": 12582912, "nnz": 2582185, "total": 12594496}, "2": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1359872, "linear_dense_total": 8388608, "linear_nnz": 1884160, "linear_total": 12582912, "nnz": 1891352, "total": 12593536}, "20": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 358400, "linear_dense_total": 8388608, "linear_nnz": 1406976, "linear_total": 12582912, "nnz": 1414063, "total": 12593920}, "21": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 194560, "linear_dense_total": 8388608, "linear_nnz": 1243136, "linear_total": 12582912, "nnz": 1250143, "total": 12593920}, "22": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 180224, "linear_dense_total": 8388608, "linear_nnz": 704512, "linear_total": 12582912, "nnz": 711128, "total": 12593536}, "23": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 323584, "linear_dense_total": 8388608, "linear_nnz": 1634304, "linear_total": 12582912, "nnz": 1641566, "total": 12594112}, "3": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 1685504, "linear_dense_total": 8388608, "linear_nnz": 2734080, "linear_total": 12582912, "nnz": 2741815, "total": 12593920}, "4": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1767424, "linear_dense_total": 8388608, "linear_nnz": 2291712, "linear_total": 12582912, "nnz": 2299103, "total": 12593536}, "5": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 1873920, "linear_dense_total": 8388608, "linear_nnz": 2660352, "linear_total": 12582912, "nnz": 2667987, "total": 12593728}, "6": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 2054144, "linear_dense_total": 8388608, "linear_nnz": 2578432, "linear_total": 12582912, "nnz": 2585963, "total": 12593536}, "7": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 1773568, "linear_dense_total": 8388608, "linear_nnz": 2822144, "linear_total": 12582912, "nnz": 2829922, "total": 12593920}, "8": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1968128, "linear_dense_total": 8388608, "linear_nnz": 2492416, "linear_total": 12582912, "nnz": 2499905, "total": 12593536}, "9": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 1986560, "linear_dense_total": 8388608, "linear_nnz": 3297280, "linear_total": 12582912, "nnz": 3305354, "total": 12594112}}, "linear_nnz": 73713664, "linear_sparsity": 75.59068467881944, "linear_total": 301989888, "nnz": 105691291, "pruned_heads": {"0": [4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 7, 8, 10, 12, 13], "11": [0, 1, 2, 4, 5, 8, 10], "12": [2, 3, 5, 6, 7, 8, 10, 13], "13": [10, 2, 3, 12], "14": [1, 2, 3, 4, 8, 11], "15": [0, 5, 6, 7, 11, 12], "16": [3, 6, 8, 10, 13, 15], "17": [0, 2, 11, 15], "18": [2, 3, 5, 9, 11, 12, 13], "19": [0, 2, 3, 4, 9, 10, 11, 13, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15], "23": [1, 2, 4, 5, 6, 7, 9, 10, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 10, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14], "6": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 8, 9, 12, 13, 15]}, "total": 334048258, "total_sparsity": 68.36047233630538}, "training_args": {"_n_gpu": -1, "adafactor": false, "adam_beta1": 0.9, "adam_beta2": 0.999, "adam_epsilon": 1e-08, "dataloader_drop_last": false, "dataloader_num_workers": 0, "dataloader_pin_memory": true, "ddp_find_unused_parameters": null, "debug": false, "deepspeed": null, "disable_tqdm": false, "do_eval": 1, "do_predict": false, "do_train": 1, "eval_accumulation_steps": null, "eval_steps": 2500, "evaluation_strategy": "steps", "fp16": false, "fp16_backend": "auto", "fp16_full_eval": false, "fp16_opt_level": "O1", "gradient_accumulation_steps": 1, "greater_is_better": null, "group_by_length": false, "ignore_data_skip": false, "label_names": null, "label_smoothing_factor": 0.0, "learning_rate": 3e-05, "load_best_model_at_end": false, "local_rank": -1, "logging_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25", "logging_first_step": false, "logging_steps": 250, "logging_strategy": "steps", "lr_scheduler_type": "linear", "max_grad_norm": 1.0, "max_steps": -1, "metric_for_best_model": null, "no_cuda": false, "num_train_epochs": 10, "optimize_model_before_eval": "disabled", "output_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25", "overwrite_output_dir": 1, "past_index": -1, "per_device_eval_batch_size": 128, "per_device_train_batch_size": 16, "per_gpu_eval_batch_size": null, "per_gpu_train_batch_size": null, "prediction_loss_only": false, "remove_unused_columns": true, "report_to": null, "run_name": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25", "save_steps": 2500, "save_strategy": "steps", "save_total_limit": 50, "seed": 17, "sharded_ddp": "", "skip_memory_metrics": false, "tpu_metrics_debug": false, "tpu_num_cores": null, "warmup_ratio": 0.0, "warmup_steps": 10, "weight_decay": 0.0}}, "f1": 91.0266636723574, "fill_rate": 0.6509150752314815, "speedup": 0.9221729963255725}}
{"speedup": 0.9261182619659336, "f1": 90.84270784891945, "meta": {"annotate": "24", "cat_fun_name": "is_new_xp", "checkpoint": {"config": {"_name_or_path": "/home/lagunas/devel/hf/nn_pruning/nn_pruning/analysis/tmp_finetune", "architectures": ["BertForQuestionAnswering"], "attention_probs_dropout_prob": 0.1, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-12, "max_position_embeddings": 512, "model_type": "bert", "num_attention_heads": 16, "num_hidden_layers": 24, "pad_token_id": 0, "position_embedding_type": "absolute", "pruned_heads": {"0": [4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 7, 8, 10, 12, 13], "11": [0, 1, 2, 4, 5, 8, 10], "12": [2, 3, 5, 6, 7, 8, 10, 13], "13": [10, 2, 3, 12], "14": [1, 2, 3, 4, 8, 11], "15": [0, 5, 6, 7, 11, 12], "16": [3, 6, 8, 10, 13, 15], "17": [0, 2, 11, 15], "18": [2, 3, 5, 9, 11, 12, 13], "19": [0, 2, 3, 4, 9, 10, 11, 13, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15], "23": [1, 2, 4, 5, 6, 7, 9, 10, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 10, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14], "6": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 8, 9, 12, 13, 15]}, "type_vocab_size": 2, "vocab_size": 30522}, "eval_metrics": {"exact_match": 84.399243140965, "f1": 90.84270784891945}, "path": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25/checkpoint-22500", "source_checkpoint": "/data_2to/devel_data/nn_pruning/output/squad_test_large/large_regu_10_d0.25/checkpoint-210000", "sparse_args": {"ampere_pruning_method": "disabled", "attention_block_cols": 1, "attention_block_rows": 1, "attention_lambda": 1.0, "attention_output_with_dense": 0, "attention_pruning_method": "topK", "bias_mask": true, "dense_block_cols": 1, "dense_block_rows": 1, "dense_lambda": 1.0, "dense_pruning_method": "topK", "distil_alpha_ce": 0.1, "distil_alpha_teacher": 0.9, "distil_teacher_name_or_path": "bert-large-uncased-whole-word-masking-finetuned-squad", "distil_temperature": 2.0, "final_ampere_temperature": 20.0, "final_finetune": 1, "final_threshold": 0.5, "final_warmup": 0, "initial_ampere_temperature": 0.0, "initial_threshold": 1.0, "initial_warmup": 0, "mask_init": "constant", "mask_scale": 0.0, "mask_scores_learning_rate": 0.01, "regularization": "", "regularization_final_lambda": 0}, "speed": {"cuda_eval_elapsed_time": 41.6732879486084, "eval_elapsed_time": 48.981834520120174}, "speedup": 0.9261182619659336, "stats": {"layers": {"0": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 835584, "linear_dense_total": 8388608, "linear_nnz": 2408448, "linear_total": 12582912, "nnz": 2416152, "total": 12594304}, "1": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1275904, "linear_dense_total": 8388608, "linear_nnz": 1800192, "linear_total": 12582912, "nnz": 1807343, "total": 12593536}, "10": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 2410496, "linear_dense_total": 8388608, "linear_nnz": 5031936, "linear_total": 12582912, "nnz": 5041177, "total": 12595072}, "11": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 2510848, "linear_dense_total": 8388608, "linear_nnz": 4870144, "linear_total": 12582912, "nnz": 4879242, "total": 12594880}, "12": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 2660352, "linear_dense_total": 8388608, "linear_nnz": 4757504, "linear_total": 12582912, "nnz": 4766483, "total": 12594688}, "13": {"linear_attention_nnz": 3145728, "linear_attention_total": 4194304, "linear_dense_nnz": 2605056, "linear_dense_total": 8388608, "linear_nnz": 5750784, "linear_total": 12582912, "nnz": 5760504, "total": 12595456}, "14": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 2299904, "linear_dense_total": 8388608, "linear_nnz": 4921344, "linear_total": 12582912, "nnz": 4930531, "total": 12595072}, "15": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 1699840, "linear_dense_total": 8388608, "linear_nnz": 4321280, "linear_total": 12582912, "nnz": 4330174, "total": 12595072}, "16": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 1402880, "linear_dense_total": 8388608, "linear_nnz": 4024320, "linear_total": 12582912, "nnz": 4033069, "total": 12595072}, "17": {"linear_attention_nnz": 3145728, "linear_attention_total": 4194304, "linear_dense_nnz": 1097728, "linear_dense_total": 8388608, "linear_nnz": 4243456, "linear_total": 12582912, "nnz": 4252440, "total": 12595456}, "18": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 901120, "linear_dense_total": 8388608, "linear_nnz": 3260416, "linear_total": 12582912, "nnz": 3268728, "total": 12594880}, "19": {"linear_attention_nnz": 1835008, "linear_attention_total": 4194304, "linear_dense_nnz": 739328, "linear_dense_total": 8388608, "linear_nnz": 2574336, "linear_total": 12582912, "nnz": 2582185, "total": 12594496}, "2": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1359872, "linear_dense_total": 8388608, "linear_nnz": 1884160, "linear_total": 12582912, "nnz": 1891352, "total": 12593536}, "20": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 358400, "linear_dense_total": 8388608, "linear_nnz": 1406976, "linear_total": 12582912, "nnz": 1414063, "total": 12593920}, "21": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 194560, "linear_dense_total": 8388608, "linear_nnz": 1243136, "linear_total": 12582912, "nnz": 1250143, "total": 12593920}, "22": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 180224, "linear_dense_total": 8388608, "linear_nnz": 704512, "linear_total": 12582912, "nnz": 711128, "total": 12593536}, "23": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 323584, "linear_dense_total": 8388608, "linear_nnz": 1634304, "linear_total": 12582912, "nnz": 1641566, "total": 12594112}, "3": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 1685504, "linear_dense_total": 8388608, "linear_nnz": 2734080, "linear_total": 12582912, "nnz": 2741815, "total": 12593920}, "4": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1767424, "linear_dense_total": 8388608, "linear_nnz": 2291712, "linear_total": 12582912, "nnz": 2299103, "total": 12593536}, "5": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 1873920, "linear_dense_total": 8388608, "linear_nnz": 2660352, "linear_total": 12582912, "nnz": 2667987, "total": 12593728}, "6": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 2054144, "linear_dense_total": 8388608, "linear_nnz": 2578432, "linear_total": 12582912, "nnz": 2585963, "total": 12593536}, "7": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 1773568, "linear_dense_total": 8388608, "linear_nnz": 2822144, "linear_total": 12582912, "nnz": 2829922, "total": 12593920}, "8": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1968128, "linear_dense_total": 8388608, "linear_nnz": 2492416, "linear_total": 12582912, "nnz": 2499905, "total": 12593536}, "9": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 1986560, "linear_dense_total": 8388608, "linear_nnz": 3297280, "linear_total": 12582912, "nnz": 3305354, "total": 12594112}}, "linear_nnz": 73713664, "linear_sparsity": 75.59068467881944, "linear_total": 301989888, "nnz": 105691291, "pruned_heads": {"0": [4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 7, 8, 10, 12, 13], "11": [0, 1, 2, 4, 5, 8, 10], "12": [2, 3, 5, 6, 7, 8, 10, 13], "13": [10, 2, 3, 12], "14": [1, 2, 3, 4, 8, 11], "15": [0, 5, 6, 7, 11, 12], "16": [3, 6, 8, 10, 13, 15], "17": [0, 2, 11, 15], "18": [2, 3, 5, 9, 11, 12, 13], "19": [0, 2, 3, 4, 9, 10, 11, 13, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15], "23": [1, 2, 4, 5, 6, 7, 9, 10, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 10, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14], "6": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 8, 9, 12, 13, 15]}, "total": 334048258, "total_sparsity": 68.36047233630538}, "training_args": {"_n_gpu": -1, "adafactor": false, "adam_beta1": 0.9, "adam_beta2": 0.999, "adam_epsilon": 1e-08, "dataloader_drop_last": false, "dataloader_num_workers": 0, "dataloader_pin_memory": true, "ddp_find_unused_parameters": null, "debug": false, "deepspeed": null, "disable_tqdm": false, "do_eval": 1, "do_predict": false, "do_train": 1, "eval_accumulation_steps": null, "eval_steps": 2500, "evaluation_strategy": "steps", "fp16": false, "fp16_backend": "auto", "fp16_full_eval": false, "fp16_opt_level": "O1", "gradient_accumulation_steps": 1, "greater_is_better": null, "group_by_length": false, "ignore_data_skip": false, "label_names": null, "label_smoothing_factor": 0.0, "learning_rate": 3e-05, "load_best_model_at_end": false, "local_rank": -1, "logging_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25", "logging_first_step": false, "logging_steps": 250, "logging_strategy": "steps", "lr_scheduler_type": "linear", "max_grad_norm": 1.0, "max_steps": -1, "metric_for_best_model": null, "no_cuda": false, "num_train_epochs": 5, "optimize_model_before_eval": "disabled", "output_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25", "overwrite_output_dir": 1, "past_index": -1, "per_device_eval_batch_size": 128, "per_device_train_batch_size": 16, "per_gpu_eval_batch_size": null, "per_gpu_train_batch_size": null, "prediction_loss_only": false, "remove_unused_columns": true, "report_to": null, "run_name": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25", "save_steps": 2500, "save_strategy": "steps", "save_total_limit": 50, "seed": 17, "sharded_ddp": "", "skip_memory_metrics": false, "tpu_metrics_debug": false, "tpu_num_cores": null, "warmup_ratio": 0.0, "warmup_steps": 10, "weight_decay": 0.0}}, "f1": 90.84270784891945, "fill_rate": 0.6509150752314815, "speedup": 0.9261182619659336}}
{"speedup": 0.929906085171529, "f1": 90.73941291394593, "meta": {"annotate": "24", "cat_fun_name": "is_new_xp", "checkpoint": {"config": {"_name_or_path": "/home/lagunas/devel/hf/nn_pruning/nn_pruning/analysis/tmp_finetune", "architectures": ["BertForQuestionAnswering"], "attention_probs_dropout_prob": 0.1, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-12, "max_position_embeddings": 512, "model_type": "bert", "num_attention_heads": 16, "num_hidden_layers": 24, "pad_token_id": 0, "position_embedding_type": "absolute", "pruned_heads": {"0": [4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 7, 8, 10, 12, 13], "11": [0, 1, 2, 4, 5, 8, 10], "12": [2, 3, 5, 6, 7, 8, 10, 13], "13": [10, 2, 3, 12], "14": [1, 2, 3, 4, 8, 11], "15": [0, 5, 6, 7, 11, 12], "16": [3, 6, 8, 10, 13, 15], "17": [0, 2, 11, 15], "18": [2, 3, 5, 9, 11, 12, 13], "19": [0, 2, 3, 4, 9, 10, 11, 13, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15], "23": [1, 2, 4, 5, 6, 7, 9, 10, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 10, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14], "6": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 8, 9, 12, 13, 15]}, "type_vocab_size": 2, "vocab_size": 30522}, "eval_metrics": {"exact_match": 84.20056764427625, "f1": 90.73941291394593}, "path": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25/checkpoint-25000", "source_checkpoint": "/data_2to/devel_data/nn_pruning/output/squad_test_large/large_regu_10_d0.25/checkpoint-210000", "sparse_args": {"ampere_pruning_method": "disabled", "attention_block_cols": 1, "attention_block_rows": 1, "attention_lambda": 1.0, "attention_output_with_dense": 0, "attention_pruning_method": "topK", "bias_mask": true, "dense_block_cols": 1, "dense_block_rows": 1, "dense_lambda": 1.0, "dense_pruning_method": "topK", "distil_alpha_ce": 0.1, "distil_alpha_teacher": 0.9, "distil_teacher_name_or_path": "bert-large-uncased-whole-word-masking-finetuned-squad", "distil_temperature": 2.0, "final_ampere_temperature": 20.0, "final_finetune": 1, "final_threshold": 0.5, "final_warmup": 0, "initial_ampere_temperature": 0.0, "initial_threshold": 1.0, "initial_warmup": 0, "mask_init": "constant", "mask_scale": 0.0, "mask_scores_learning_rate": 0.01, "regularization": "", "regularization_final_lambda": 0}, "speed": {"cuda_eval_elapsed_time": 41.50353849792481, "eval_elapsed_time": 49.06402187002823}, "speedup": 0.929906085171529, "stats": {"layers": {"0": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 835584, "linear_dense_total": 8388608, "linear_nnz": 2408448, "linear_total": 12582912, "nnz": 2416152, "total": 12594304}, "1": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1275904, "linear_dense_total": 8388608, "linear_nnz": 1800192, "linear_total": 12582912, "nnz": 1807343, "total": 12593536}, "10": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 2410496, "linear_dense_total": 8388608, "linear_nnz": 5031936, "linear_total": 12582912, "nnz": 5041177, "total": 12595072}, "11": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 2510848, "linear_dense_total": 8388608, "linear_nnz": 4870144, "linear_total": 12582912, "nnz": 4879242, "total": 12594880}, "12": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 2660352, "linear_dense_total": 8388608, "linear_nnz": 4757504, "linear_total": 12582912, "nnz": 4766483, "total": 12594688}, "13": {"linear_attention_nnz": 3145728, "linear_attention_total": 4194304, "linear_dense_nnz": 2605056, "linear_dense_total": 8388608, "linear_nnz": 5750784, "linear_total": 12582912, "nnz": 5760504, "total": 12595456}, "14": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 2299904, "linear_dense_total": 8388608, "linear_nnz": 4921344, "linear_total": 12582912, "nnz": 4930531, "total": 12595072}, "15": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 1699840, "linear_dense_total": 8388608, "linear_nnz": 4321280, "linear_total": 12582912, "nnz": 4330174, "total": 12595072}, "16": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 1402880, "linear_dense_total": 8388608, "linear_nnz": 4024320, "linear_total": 12582912, "nnz": 4033069, "total": 12595072}, "17": {"linear_attention_nnz": 3145728, "linear_attention_total": 4194304, "linear_dense_nnz": 1097728, "linear_dense_total": 8388608, "linear_nnz": 4243456, "linear_total": 12582912, "nnz": 4252440, "total": 12595456}, "18": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 901120, "linear_dense_total": 8388608, "linear_nnz": 3260416, "linear_total": 12582912, "nnz": 3268728, "total": 12594880}, "19": {"linear_attention_nnz": 1835008, "linear_attention_total": 4194304, "linear_dense_nnz": 739328, "linear_dense_total": 8388608, "linear_nnz": 2574336, "linear_total": 12582912, "nnz": 2582185, "total": 12594496}, "2": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1359872, "linear_dense_total": 8388608, "linear_nnz": 1884160, "linear_total": 12582912, "nnz": 1891352, "total": 12593536}, "20": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 358400, "linear_dense_total": 8388608, "linear_nnz": 1406976, "linear_total": 12582912, "nnz": 1414063, "total": 12593920}, "21": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 194560, "linear_dense_total": 8388608, "linear_nnz": 1243136, "linear_total": 12582912, "nnz": 1250143, "total": 12593920}, "22": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 180224, "linear_dense_total": 8388608, "linear_nnz": 704512, "linear_total": 12582912, "nnz": 711128, "total": 12593536}, "23": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 323584, "linear_dense_total": 8388608, "linear_nnz": 1634304, "linear_total": 12582912, "nnz": 1641566, "total": 12594112}, "3": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 1685504, "linear_dense_total": 8388608, "linear_nnz": 2734080, "linear_total": 12582912, "nnz": 2741815, "total": 12593920}, "4": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1767424, "linear_dense_total": 8388608, "linear_nnz": 2291712, "linear_total": 12582912, "nnz": 2299103, "total": 12593536}, "5": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 1873920, "linear_dense_total": 8388608, "linear_nnz": 2660352, "linear_total": 12582912, "nnz": 2667987, "total": 12593728}, "6": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 2054144, "linear_dense_total": 8388608, "linear_nnz": 2578432, "linear_total": 12582912, "nnz": 2585963, "total": 12593536}, "7": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 1773568, "linear_dense_total": 8388608, "linear_nnz": 2822144, "linear_total": 12582912, "nnz": 2829922, "total": 12593920}, "8": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 1968128, "linear_dense_total": 8388608, "linear_nnz": 2492416, "linear_total": 12582912, "nnz": 2499905, "total": 12593536}, "9": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 1986560, "linear_dense_total": 8388608, "linear_nnz": 3297280, "linear_total": 12582912, "nnz": 3305354, "total": 12594112}}, "linear_nnz": 73713664, "linear_sparsity": 75.59068467881944, "linear_total": 301989888, "nnz": 105691291, "pruned_heads": {"0": [4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 7, 8, 10, 12, 13], "11": [0, 1, 2, 4, 5, 8, 10], "12": [2, 3, 5, 6, 7, 8, 10, 13], "13": [10, 2, 3, 12], "14": [1, 2, 3, 4, 8, 11], "15": [0, 5, 6, 7, 11, 12], "16": [3, 6, 8, 10, 13, 15], "17": [0, 2, 11, 15], "18": [2, 3, 5, 9, 11, 12, 13], "19": [0, 2, 3, 4, 9, 10, 11, 13, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15], "23": [1, 2, 4, 5, 6, 7, 9, 10, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 10, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14], "6": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 8, 9, 12, 13, 15]}, "total": 334048258, "total_sparsity": 68.36047233630538}, "training_args": {"_n_gpu": -1, "adafactor": false, "adam_beta1": 0.9, "adam_beta2": 0.999, "adam_epsilon": 1e-08, "dataloader_drop_last": false, "dataloader_num_workers": 0, "dataloader_pin_memory": true, "ddp_find_unused_parameters": null, "debug": false, "deepspeed": null, "disable_tqdm": false, "do_eval": 1, "do_predict": false, "do_train": 1, "eval_accumulation_steps": null, "eval_steps": 2500, "evaluation_strategy": "steps", "fp16": false, "fp16_backend": "auto", "fp16_full_eval": false, "fp16_opt_level": "O1", "gradient_accumulation_steps": 1, "greater_is_better": null, "group_by_length": false, "ignore_data_skip": false, "label_names": null, "label_smoothing_factor": 0.0, "learning_rate": 3e-05, "load_best_model_at_end": false, "local_rank": -1, "logging_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25", "logging_first_step": false, "logging_steps": 250, "logging_strategy": "steps", "lr_scheduler_type": "linear", "max_grad_norm": 1.0, "max_steps": -1, "metric_for_best_model": null, "no_cuda": false, "num_train_epochs": 5, "optimize_model_before_eval": "disabled", "output_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25", "overwrite_output_dir": 1, "past_index": -1, "per_device_eval_batch_size": 128, "per_device_train_batch_size": 16, "per_gpu_eval_batch_size": null, "per_gpu_train_batch_size": null, "prediction_loss_only": false, "remove_unused_columns": true, "report_to": null, "run_name": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10_d0.25", "save_steps": 2500, "save_strategy": "steps", "save_total_limit": 50, "seed": 17, "sharded_ddp": "", "skip_memory_metrics": false, "tpu_metrics_debug": false, "tpu_num_cores": null, "warmup_ratio": 0.0, "warmup_steps": 10, "weight_decay": 0.0}}, "f1": 90.73941291394593, "fill_rate": 0.6509150752314815, "speedup": 0.929906085171529}}
{"speedup": 1.0281280670181348, "f1": 90.16320537561052, "meta": {"annotate": "17", "cat_fun_name": "is_new_xp", "checkpoint": {"config": {"_name_or_path": "/home/lagunas/devel/hf/nn_pruning/nn_pruning/analysis/tmp_finetune", "architectures": ["BertForQuestionAnswering"], "attention_probs_dropout_prob": 0.1, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-12, "max_position_embeddings": 512, "model_type": "bert", "num_attention_heads": 16, "num_hidden_layers": 24, "pad_token_id": 0, "position_embedding_type": "absolute", "pruned_heads": {"0": [2, 3, 4, 7, 8, 9, 10, 13, 14, 15], "1": [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [7, 8, 10, 12, 13, 14], "11": [0, 2, 4, 5, 8, 10], "12": [10, 3, 13, 6], "13": [2, 10, 4, 12], "14": [2, 3, 4, 8, 11], "15": [0, 5, 6, 7, 11, 12], "16": [3, 6, 8, 13, 15], "17": [0, 2, 4, 11, 15], "18": [2, 3, 5, 11, 13], "19": [0, 2, 3, 4, 9, 10, 11, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [2, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14], "23": [1, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 10, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 14], "8": [0, 1, 2, 3, 4, 5, 6, 8, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 8, 12, 13, 15]}, "type_vocab_size": 2, "vocab_size": 30522}, "eval_metrics": {"exact_match": 83.74645222327341, "f1": 90.16320537561052}, "path": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10/checkpoint-47500", "source_checkpoint": "/data_2to/devel_data/nn_pruning/output/squad_test_large/large_regu_10/checkpoint-215000", "sparse_args": {"ampere_pruning_method": "disabled", "attention_block_cols": 1, "attention_block_rows": 1, "attention_lambda": 1.0, "attention_output_with_dense": 0, "attention_pruning_method": "topK", "bias_mask": true, "dense_block_cols": 1, "dense_block_rows": 1, "dense_lambda": 1.0, "dense_pruning_method": "topK", "distil_alpha_ce": 0.1, "distil_alpha_teacher": 0.9, "distil_teacher_name_or_path": "bert-large-uncased-whole-word-masking-finetuned-squad", "distil_temperature": 2.0, "final_ampere_temperature": 20.0, "final_finetune": 1, "final_threshold": 0.5, "final_warmup": 0, "initial_ampere_temperature": 0.0, "initial_threshold": 1.0, "initial_warmup": 0, "mask_init": "constant", "mask_scale": 0.0, "mask_scores_learning_rate": 0.01, "regularization": "", "regularization_final_lambda": 0}, "speed": {"cuda_eval_elapsed_time": 37.53850735473633, "eval_elapsed_time": 44.58338421070948}, "speedup": 1.0281280670181348, "stats": {"layers": {"0": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 192512, "linear_dense_total": 8388608, "linear_nnz": 1765376, "linear_total": 12582912, "nnz": 1772766, "total": 12594304}, "1": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 270336, "linear_dense_total": 8388608, "linear_nnz": 794624, "linear_total": 12582912, "nnz": 801284, "total": 12593536}, "10": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 995328, "linear_dense_total": 8388608, "linear_nnz": 3616768, "linear_total": 12582912, "nnz": 3625318, "total": 12595072}, "11": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 1032192, "linear_dense_total": 8388608, "linear_nnz": 3653632, "linear_total": 12582912, "nnz": 3662200, "total": 12595072}, "12": {"linear_attention_nnz": 3145728, "linear_attention_total": 4194304, "linear_dense_nnz": 1241088, "linear_dense_total": 8388608, "linear_nnz": 4386816, "linear_total": 12582912, "nnz": 4395870, "total": 12595456}, "13": {"linear_attention_nnz": 3145728, "linear_attention_total": 4194304, "linear_dense_nnz": 1179648, "linear_dense_total": 8388608, "linear_nnz": 4325376, "linear_total": 12582912, "nnz": 4334400, "total": 12595456}, "14": {"linear_attention_nnz": 2883584, "linear_attention_total": 4194304, "linear_dense_nnz": 909312, "linear_dense_total": 8388608, "linear_nnz": 3792896, "linear_total": 12582912, "nnz": 3801596, "total": 12595264}, "15": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 681984, "linear_dense_total": 8388608, "linear_nnz": 3303424, "linear_total": 12582912, "nnz": 3311821, "total": 12595072}, "16": {"linear_attention_nnz": 2883584, "linear_attention_total": 4194304, "linear_dense_nnz": 473088, "linear_dense_total": 8388608, "linear_nnz": 3356672, "linear_total": 12582912, "nnz": 3365159, "total": 12595264}, "17": {"linear_attention_nnz": 2883584, "linear_attention_total": 4194304, "linear_dense_nnz": 368640, "linear_dense_total": 8388608, "linear_nnz": 3252224, "linear_total": 12582912, "nnz": 3260660, "total": 12595264}, "18": {"linear_attention_nnz": 2883584, "linear_attention_total": 4194304, "linear_dense_nnz": 321536, "linear_dense_total": 8388608, "linear_nnz": 3205120, "linear_total": 12582912, "nnz": 3213533, "total": 12595264}, "19": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 270336, "linear_dense_total": 8388608, "linear_nnz": 2367488, "linear_total": 12582912, "nnz": 2375300, "total": 12594688}, "2": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 286720, "linear_dense_total": 8388608, "linear_nnz": 811008, "linear_total": 12582912, "nnz": 817676, "total": 12593536}, "20": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 112640, "linear_dense_total": 8388608, "linear_nnz": 899072, "linear_total": 12582912, "nnz": 905847, "total": 12593728}, "21": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 77824, "linear_dense_total": 8388608, "linear_nnz": 1388544, "linear_total": 12582912, "nnz": 1395686, "total": 12594112}, "22": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 79872, "linear_dense_total": 8388608, "linear_nnz": 866304, "linear_total": 12582912, "nnz": 873063, "total": 12593728}, "23": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 182272, "linear_dense_total": 8388608, "linear_nnz": 1230848, "linear_total": 12582912, "nnz": 1237849, "total": 12593920}, "3": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 413696, "linear_dense_total": 8388608, "linear_nnz": 1724416, "linear_total": 12582912, "nnz": 1731722, "total": 12594112}, "4": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 466944, "linear_dense_total": 8388608, "linear_nnz": 991232, "linear_total": 12582912, "nnz": 997988, "total": 12593536}, "5": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 552960, "linear_dense_total": 8388608, "linear_nnz": 1077248, "linear_total": 12582912, "nnz": 1084046, "total": 12593536}, "6": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 608256, "linear_dense_total": 8388608, "linear_nnz": 1394688, "linear_total": 12582912, "nnz": 1401705, "total": 12593728}, "7": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 438272, "linear_dense_total": 8388608, "linear_nnz": 1748992, "linear_total": 12582912, "nnz": 1756310, "total": 12594112}, "8": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 661504, "linear_dense_total": 8388608, "linear_nnz": 1710080, "linear_total": 12582912, "nnz": 1717315, "total": 12593920}, "9": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 747520, "linear_dense_total": 8388608, "linear_nnz": 2320384, "linear_total": 12582912, "nnz": 2328045, "total": 12594304}}, "linear_nnz": 53983232, "linear_sparsity": 82.12415907118056, "linear_total": 301989888, "nnz": 85952121, "pruned_heads": {"0": [2, 3, 4, 7, 8, 9, 10, 13, 14, 15], "1": [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [7, 8, 10, 12, 13, 14], "11": [0, 2, 4, 5, 8, 10], "12": [10, 3, 13, 6], "13": [2, 10, 4, 12], "14": [2, 3, 4, 8, 11], "15": [0, 5, 6, 7, 11, 12], "16": [3, 6, 8, 13, 15], "17": [0, 2, 4, 11, 15], "18": [2, 3, 5, 11, 13], "19": [0, 2, 3, 4, 9, 10, 11, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [2, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14], "23": [1, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 10, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 14], "8": [0, 1, 2, 3, 4, 5, 6, 8, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 8, 12, 13, 15]}, "total": 334050946, "total_sparsity": 74.26975674542769}, "training_args": {"_n_gpu": -1, "adafactor": false, "adam_beta1": 0.9, "adam_beta2": 0.999, "adam_epsilon": 1e-08, "dataloader_drop_last": false, "dataloader_num_workers": 0, "dataloader_pin_memory": true, "ddp_find_unused_parameters": null, "debug": false, "deepspeed": null, "disable_tqdm": false, "do_eval": 1, "do_predict": false, "do_train": 1, "eval_accumulation_steps": null, "eval_steps": 2500, "evaluation_strategy": "steps", "fp16": false, "fp16_backend": "auto", "fp16_full_eval": false, "fp16_opt_level": "O1", "gradient_accumulation_steps": 1, "greater_is_better": null, "group_by_length": false, "ignore_data_skip": false, "label_names": null, "label_smoothing_factor": 0.0, "learning_rate": 3e-05, "load_best_model_at_end": false, "local_rank": -1, "logging_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10", "logging_first_step": false, "logging_steps": 250, "logging_strategy": "steps", "lr_scheduler_type": "linear", "max_grad_norm": 1.0, "max_steps": -1, "metric_for_best_model": null, "no_cuda": false, "num_train_epochs": 10, "optimize_model_before_eval": "disabled", "output_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10", "overwrite_output_dir": 1, "past_index": -1, "per_device_eval_batch_size": 128, "per_device_train_batch_size": 16, "per_gpu_eval_batch_size": null, "per_gpu_train_batch_size": null, "prediction_loss_only": false, "remove_unused_columns": true, "report_to": null, "run_name": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10", "save_steps": 2500, "save_strategy": "steps", "save_total_limit": 50, "seed": 17, "sharded_ddp": "", "skip_memory_metrics": false, "tpu_metrics_debug": false, "tpu_num_cores": null, "warmup_ratio": 0.0, "warmup_steps": 10, "weight_decay": 0.0}}, "f1": 90.16320537561052, "fill_rate": 0.4766890914351851, "speedup": 1.0281280670181348}}
{"speedup": 1.034699920808227, "f1": 90.10843526218638, "meta": {"annotate": "17", "cat_fun_name": "is_new_xp", "checkpoint": {"config": {"_name_or_path": "/home/lagunas/devel/hf/nn_pruning/nn_pruning/analysis/tmp_finetune", "architectures": ["BertForQuestionAnswering"], "attention_probs_dropout_prob": 0.1, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-12, "max_position_embeddings": 512, "model_type": "bert", "num_attention_heads": 16, "num_hidden_layers": 24, "pad_token_id": 0, "position_embedding_type": "absolute", "pruned_heads": {"0": [2, 3, 4, 7, 8, 9, 10, 13, 14, 15], "1": [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [7, 8, 10, 12, 13, 14], "11": [0, 2, 4, 5, 8, 10], "12": [10, 3, 13, 6], "13": [2, 10, 4, 12], "14": [2, 3, 4, 8, 11], "15": [0, 5, 6, 7, 11, 12], "16": [3, 6, 8, 13, 15], "17": [0, 2, 4, 11, 15], "18": [2, 3, 5, 11, 13], "19": [0, 2, 3, 4, 9, 10, 11, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [2, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14], "23": [1, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 10, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 14], "8": [0, 1, 2, 3, 4, 5, 6, 8, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 8, 12, 13, 15]}, "type_vocab_size": 2, "vocab_size": 30522}, "eval_metrics": {"exact_match": 83.62346263008514, "f1": 90.10843526218638}, "path": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10/checkpoint-55330", "source_checkpoint": "/data_2to/devel_data/nn_pruning/output/squad_test_large/large_regu_10/checkpoint-215000", "sparse_args": {"ampere_pruning_method": "disabled", "attention_block_cols": 1, "attention_block_rows": 1, "attention_lambda": 1.0, "attention_output_with_dense": 0, "attention_pruning_method": "topK", "bias_mask": true, "dense_block_cols": 1, "dense_block_rows": 1, "dense_lambda": 1.0, "dense_pruning_method": "topK", "distil_alpha_ce": 0.1, "distil_alpha_teacher": 0.9, "distil_teacher_name_or_path": "bert-large-uncased-whole-word-masking-finetuned-squad", "distil_temperature": 2.0, "final_ampere_temperature": 20.0, "final_finetune": 1, "final_threshold": 0.5, "final_warmup": 0, "initial_ampere_temperature": 0.0, "initial_threshold": 1.0, "initial_warmup": 0, "mask_init": "constant", "mask_scale": 0.0, "mask_scores_learning_rate": 0.01, "regularization": "", "regularization_final_lambda": 0}, "speed": {"cuda_eval_elapsed_time": 37.30008307647705, "eval_elapsed_time": 44.469506811816245}, "speedup": 1.034699920808227, "stats": {"layers": {"0": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 192512, "linear_dense_total": 8388608, "linear_nnz": 1765376, "linear_total": 12582912, "nnz": 1772766, "total": 12594304}, "1": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 270336, "linear_dense_total": 8388608, "linear_nnz": 794624, "linear_total": 12582912, "nnz": 801284, "total": 12593536}, "10": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 995328, "linear_dense_total": 8388608, "linear_nnz": 3616768, "linear_total": 12582912, "nnz": 3625318, "total": 12595072}, "11": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 1032192, "linear_dense_total": 8388608, "linear_nnz": 3653632, "linear_total": 12582912, "nnz": 3662200, "total": 12595072}, "12": {"linear_attention_nnz": 3145728, "linear_attention_total": 4194304, "linear_dense_nnz": 1241088, "linear_dense_total": 8388608, "linear_nnz": 4386816, "linear_total": 12582912, "nnz": 4395870, "total": 12595456}, "13": {"linear_attention_nnz": 3145728, "linear_attention_total": 4194304, "linear_dense_nnz": 1179648, "linear_dense_total": 8388608, "linear_nnz": 4325376, "linear_total": 12582912, "nnz": 4334400, "total": 12595456}, "14": {"linear_attention_nnz": 2883584, "linear_attention_total": 4194304, "linear_dense_nnz": 909312, "linear_dense_total": 8388608, "linear_nnz": 3792896, "linear_total": 12582912, "nnz": 3801596, "total": 12595264}, "15": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 681984, "linear_dense_total": 8388608, "linear_nnz": 3303424, "linear_total": 12582912, "nnz": 3311821, "total": 12595072}, "16": {"linear_attention_nnz": 2883584, "linear_attention_total": 4194304, "linear_dense_nnz": 473088, "linear_dense_total": 8388608, "linear_nnz": 3356672, "linear_total": 12582912, "nnz": 3365159, "total": 12595264}, "17": {"linear_attention_nnz": 2883584, "linear_attention_total": 4194304, "linear_dense_nnz": 368640, "linear_dense_total": 8388608, "linear_nnz": 3252224, "linear_total": 12582912, "nnz": 3260660, "total": 12595264}, "18": {"linear_attention_nnz": 2883584, "linear_attention_total": 4194304, "linear_dense_nnz": 321536, "linear_dense_total": 8388608, "linear_nnz": 3205120, "linear_total": 12582912, "nnz": 3213533, "total": 12595264}, "19": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 270336, "linear_dense_total": 8388608, "linear_nnz": 2367488, "linear_total": 12582912, "nnz": 2375300, "total": 12594688}, "2": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 286720, "linear_dense_total": 8388608, "linear_nnz": 811008, "linear_total": 12582912, "nnz": 817676, "total": 12593536}, "20": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 112640, "linear_dense_total": 8388608, "linear_nnz": 899072, "linear_total": 12582912, "nnz": 905847, "total": 12593728}, "21": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 77824, "linear_dense_total": 8388608, "linear_nnz": 1388544, "linear_total": 12582912, "nnz": 1395686, "total": 12594112}, "22": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 79872, "linear_dense_total": 8388608, "linear_nnz": 866304, "linear_total": 12582912, "nnz": 873063, "total": 12593728}, "23": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 182272, "linear_dense_total": 8388608, "linear_nnz": 1230848, "linear_total": 12582912, "nnz": 1237849, "total": 12593920}, "3": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 413696, "linear_dense_total": 8388608, "linear_nnz": 1724416, "linear_total": 12582912, "nnz": 1731722, "total": 12594112}, "4": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 466944, "linear_dense_total": 8388608, "linear_nnz": 991232, "linear_total": 12582912, "nnz": 997988, "total": 12593536}, "5": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 552960, "linear_dense_total": 8388608, "linear_nnz": 1077248, "linear_total": 12582912, "nnz": 1084046, "total": 12593536}, "6": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 608256, "linear_dense_total": 8388608, "linear_nnz": 1394688, "linear_total": 12582912, "nnz": 1401705, "total": 12593728}, "7": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 438272, "linear_dense_total": 8388608, "linear_nnz": 1748992, "linear_total": 12582912, "nnz": 1756310, "total": 12594112}, "8": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 661504, "linear_dense_total": 8388608, "linear_nnz": 1710080, "linear_total": 12582912, "nnz": 1717315, "total": 12593920}, "9": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 747520, "linear_dense_total": 8388608, "linear_nnz": 2320384, "linear_total": 12582912, "nnz": 2328045, "total": 12594304}}, "linear_nnz": 53983232, "linear_sparsity": 82.12415907118056, "linear_total": 301989888, "nnz": 85952121, "pruned_heads": {"0": [2, 3, 4, 7, 8, 9, 10, 13, 14, 15], "1": [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [7, 8, 10, 12, 13, 14], "11": [0, 2, 4, 5, 8, 10], "12": [10, 3, 13, 6], "13": [2, 10, 4, 12], "14": [2, 3, 4, 8, 11], "15": [0, 5, 6, 7, 11, 12], "16": [3, 6, 8, 13, 15], "17": [0, 2, 4, 11, 15], "18": [2, 3, 5, 11, 13], "19": [0, 2, 3, 4, 9, 10, 11, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [2, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14], "23": [1, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 10, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 14], "8": [0, 1, 2, 3, 4, 5, 6, 8, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 8, 12, 13, 15]}, "total": 334050946, "total_sparsity": 74.26975674542769}, "training_args": {"_n_gpu": -1, "adafactor": false, "adam_beta1": 0.9, "adam_beta2": 0.999, "adam_epsilon": 1e-08, "dataloader_drop_last": false, "dataloader_num_workers": 0, "dataloader_pin_memory": true, "ddp_find_unused_parameters": null, "debug": false, "deepspeed": null, "disable_tqdm": false, "do_eval": 1, "do_predict": false, "do_train": 1, "eval_accumulation_steps": null, "eval_steps": 2500, "evaluation_strategy": "steps", "fp16": false, "fp16_backend": "auto", "fp16_full_eval": false, "fp16_opt_level": "O1", "gradient_accumulation_steps": 1, "greater_is_better": null, "group_by_length": false, "ignore_data_skip": false, "label_names": null, "label_smoothing_factor": 0.0, "learning_rate": 3e-05, "load_best_model_at_end": false, "local_rank": -1, "logging_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10", "logging_first_step": false, "logging_steps": 250, "logging_strategy": "steps", "lr_scheduler_type": "linear", "max_grad_norm": 1.0, "max_steps": -1, "metric_for_best_model": null, "no_cuda": false, "num_train_epochs": 10, "optimize_model_before_eval": "disabled", "output_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10", "overwrite_output_dir": 1, "past_index": -1, "per_device_eval_batch_size": 128, "per_device_train_batch_size": 16, "per_gpu_eval_batch_size": null, "per_gpu_train_batch_size": null, "prediction_loss_only": false, "remove_unused_columns": true, "report_to": null, "run_name": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_10", "save_steps": 2500, "save_strategy": "steps", "save_total_limit": 50, "seed": 17, "sharded_ddp": "", "skip_memory_metrics": false, "tpu_metrics_debug": false, "tpu_num_cores": null, "warmup_ratio": 0.0, "warmup_steps": 10, "weight_decay": 0.0}}, "f1": 90.10843526218638, "fill_rate": 0.4766890914351851, "speedup": 1.034699920808227}}
{"speedup": 1.2874356761138743, "f1": 89.39825688878855, "meta": {"annotate": "14", "cat_fun_name": "is_new_xp", "checkpoint": {"config": {"_name_or_path": "/home/lagunas/devel/hf/nn_pruning/nn_pruning/analysis/tmp_finetune", "architectures": ["BertForQuestionAnswering"], "attention_probs_dropout_prob": 0.1, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-12, "max_position_embeddings": 512, "model_type": "bert", "num_attention_heads": 16, "num_hidden_layers": 24, "pad_token_id": 0, "position_embedding_type": "absolute", "pruned_heads": {"0": [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 3, 6, 7, 8, 10, 12, 13, 14], "11": [0, 1, 2, 4, 5, 6, 7, 8, 10, 12, 15], "12": [2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14], "13": [2, 3, 4, 10, 11, 12], "14": [1, 2, 3, 4, 8, 9, 11, 13], "15": [0, 1, 2, 5, 6, 7, 8, 9, 11, 12], "16": [3, 6, 7, 8, 10, 12, 13, 15], "17": [0, 2, 4, 11, 12, 15], "18": [2, 3, 5, 9, 11, 12, 13], "19": [0, 1, 2, 3, 4, 5, 9, 10, 11, 13, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15], "21": [0, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14], "23": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15]}, "type_vocab_size": 2, "vocab_size": 30522}, "eval_metrics": {"exact_match": 82.32734153263955, "f1": 89.39825688878855}, "path": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_40_d0.25/checkpoint-52500", "source_checkpoint": "/data_2to/devel_data/nn_pruning/output/squad_test_large/large_regu_40_d0.25/checkpoint-220000", "sparse_args": {"ampere_pruning_method": "disabled", "attention_block_cols": 1, "attention_block_rows": 1, "attention_lambda": 1.0, "attention_output_with_dense": 0, "attention_pruning_method": "topK", "bias_mask": true, "dense_block_cols": 1, "dense_block_rows": 1, "dense_lambda": 1.0, "dense_pruning_method": "topK", "distil_alpha_ce": 0.1, "distil_alpha_teacher": 0.9, "distil_teacher_name_or_path": "bert-large-uncased-whole-word-masking-finetuned-squad", "distil_temperature": 2.0, "final_ampere_temperature": 20.0, "final_finetune": 1, "final_threshold": 0.5, "final_warmup": 0, "initial_ampere_temperature": 0.0, "initial_threshold": 1.0, "initial_warmup": 0, "mask_init": "constant", "mask_scale": 0.0, "mask_scores_learning_rate": 0.01, "regularization": "", "regularization_final_lambda": 0}, "speed": {"cuda_eval_elapsed_time": 29.977725273132325, "eval_elapsed_time": 37.05464425915852}, "speedup": 1.2874356761138743, "stats": {"layers": {"0": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 253952, "linear_dense_total": 8388608, "linear_nnz": 778240, "linear_total": 12582912, "nnz": 784892, "total": 12593536}, "1": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 432128, "linear_dense_total": 8388608, "linear_nnz": 956416, "linear_total": 12582912, "nnz": 963155, "total": 12593536}, "10": {"linear_attention_nnz": 1835008, "linear_attention_total": 4194304, "linear_dense_nnz": 1210368, "linear_dense_total": 8388608, "linear_nnz": 3045376, "linear_total": 12582912, "nnz": 3053455, "total": 12594496}, "11": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 1277952, "linear_dense_total": 8388608, "linear_nnz": 2588672, "linear_total": 12582912, "nnz": 2596400, "total": 12594112}, "12": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 1400832, "linear_dense_total": 8388608, "linear_nnz": 2711552, "linear_total": 12582912, "nnz": 2719340, "total": 12594112}, "13": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 1464320, "linear_dense_total": 8388608, "linear_nnz": 4085760, "linear_total": 12582912, "nnz": 4094539, "total": 12595072}, "14": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 1122304, "linear_dense_total": 8388608, "linear_nnz": 3219456, "linear_total": 12582912, "nnz": 3227684, "total": 12594688}, "15": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 778240, "linear_dense_total": 8388608, "linear_nnz": 2351104, "linear_total": 12582912, "nnz": 2358780, "total": 12594304}, "16": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 532480, "linear_dense_total": 8388608, "linear_nnz": 2629632, "linear_total": 12582912, "nnz": 2637572, "total": 12594688}, "17": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 456704, "linear_dense_total": 8388608, "linear_nnz": 3078144, "linear_total": 12582912, "nnz": 3086431, "total": 12595072}, "18": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 440320, "linear_dense_total": 8388608, "linear_nnz": 2799616, "linear_total": 12582912, "nnz": 2807703, "total": 12594880}, "19": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 362496, "linear_dense_total": 8388608, "linear_nnz": 1673216, "linear_total": 12582912, "nnz": 1680497, "total": 12594112}, "2": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 450560, "linear_dense_total": 8388608, "linear_nnz": 974848, "linear_total": 12582912, "nnz": 981596, "total": 12593536}, "20": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 184320, "linear_dense_total": 8388608, "linear_nnz": 708608, "linear_total": 12582912, "nnz": 715226, "total": 12593536}, "21": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 112640, "linear_dense_total": 8388608, "linear_nnz": 899072, "linear_total": 12582912, "nnz": 905847, "total": 12593728}, "22": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 114688, "linear_dense_total": 8388608, "linear_nnz": 901120, "linear_total": 12582912, "nnz": 907896, "total": 12593728}, "23": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 184320, "linear_dense_total": 8388608, "linear_nnz": 708608, "linear_total": 12582912, "nnz": 715226, "total": 12593536}, "3": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 548864, "linear_dense_total": 8388608, "linear_nnz": 1073152, "linear_total": 12582912, "nnz": 1079948, "total": 12593536}, "4": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 614400, "linear_dense_total": 8388608, "linear_nnz": 1138688, "linear_total": 12582912, "nnz": 1145516, "total": 12593536}, "5": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 839680, "linear_dense_total": 8388608, "linear_nnz": 1101824, "linear_total": 12582912, "nnz": 1108570, "total": 12593344}, "6": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 858112, "linear_dense_total": 8388608, "linear_nnz": 1120256, "linear_total": 12582912, "nnz": 1127011, "total": 12593344}, "7": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 636928, "linear_dense_total": 8388608, "linear_nnz": 1423360, "linear_total": 12582912, "nnz": 1430391, "total": 12593728}, "8": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 847872, "linear_dense_total": 8388608, "linear_nnz": 1372160, "linear_total": 12582912, "nnz": 1379102, "total": 12593536}, "9": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 901120, "linear_dense_total": 8388608, "linear_nnz": 1687552, "linear_total": 12582912, "nnz": 1694712, "total": 12593728}}, "linear_nnz": 43026432, "linear_sparsity": 85.75236002604166, "linear_total": 301989888, "nnz": 74986451, "pruned_heads": {"0": [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 3, 6, 7, 8, 10, 12, 13, 14], "11": [0, 1, 2, 4, 5, 6, 7, 8, 10, 12, 15], "12": [2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14], "13": [2, 3, 4, 10, 11, 12], "14": [1, 2, 3, 4, 8, 9, 11, 13], "15": [0, 1, 2, 5, 6, 7, 8, 9, 11, 12], "16": [3, 6, 7, 8, 10, 12, 13, 15], "17": [0, 2, 4, 11, 12, 15], "18": [2, 3, 5, 9, 11, 12, 13], "19": [0, 1, 2, 3, 4, 5, 9, 10, 11, 13, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15], "21": [0, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14], "23": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15]}, "total": 334040386, "total_sparsity": 77.5516811311552}, "training_args": {"_n_gpu": -1, "adafactor": false, "adam_beta1": 0.9, "adam_beta2": 0.999, "adam_epsilon": 1e-08, "dataloader_drop_last": false, "dataloader_num_workers": 0, "dataloader_pin_memory": true, "ddp_find_unused_parameters": null, "debug": false, "deepspeed": null, "disable_tqdm": false, "do_eval": 1, "do_predict": false, "do_train": 1, "eval_accumulation_steps": null, "eval_steps": 2500, "evaluation_strategy": "steps", "fp16": false, "fp16_backend": "auto", "fp16_full_eval": false, "fp16_opt_level": "O1", "gradient_accumulation_steps": 1, "greater_is_better": null, "group_by_length": false, "ignore_data_skip": false, "label_names": null, "label_smoothing_factor": 0.0, "learning_rate": 3e-05, "load_best_model_at_end": false, "local_rank": -1, "logging_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_40_d0.25", "logging_first_step": false, "logging_steps": 250, "logging_strategy": "steps", "lr_scheduler_type": "linear", "max_grad_norm": 1.0, "max_steps": -1, "metric_for_best_model": null, "no_cuda": false, "num_train_epochs": 10, "optimize_model_before_eval": "disabled", "output_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_40_d0.25", "overwrite_output_dir": 1, "past_index": -1, "per_device_eval_batch_size": 128, "per_device_train_batch_size": 16, "per_gpu_eval_batch_size": null, "per_gpu_train_batch_size": null, "prediction_loss_only": false, "remove_unused_columns": true, "report_to": null, "run_name": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_40_d0.25", "save_steps": 2500, "save_strategy": "steps", "save_total_limit": 50, "seed": 17, "sharded_ddp": "", "skip_memory_metrics": false, "tpu_metrics_debug": false, "tpu_num_cores": null, "warmup_ratio": 0.0, "warmup_steps": 10, "weight_decay": 0.0}}, "f1": 89.39825688878855, "fill_rate": 0.3799370659722226, "speedup": 1.2874356761138743}}
{"speedup": 1.4102258637932692, "f1": 88.43605833215561, "meta": {"annotate": "12", "cat_fun_name": "is_new_xp", "checkpoint": {"config": {"_name_or_path": "/home/lagunas/devel/hf/nn_pruning/nn_pruning/analysis/tmp_finetune", "architectures": ["BertForQuestionAnswering"], "attention_probs_dropout_prob": 0.1, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-12, "max_position_embeddings": 512, "model_type": "bert", "num_attention_heads": 16, "num_hidden_layers": 24, "pad_token_id": 0, "position_embedding_type": "absolute", "pruned_heads": {"0": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 3, 6, 7, 8, 10, 12, 13, 14, 15], "11": [0, 1, 2, 4, 5, 6, 7, 8, 10, 12, 15], "12": [1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14], "13": [2, 3, 4, 9, 10, 11, 12, 13], "14": [1, 2, 3, 4, 8, 9, 11, 12], "15": [0, 1, 2, 5, 6, 7, 8, 9, 11, 12], "16": [3, 6, 7, 8, 10, 12, 13, 15], "17": [0, 2, 4, 8, 11, 12, 15], "18": [2, 3, 5, 9, 10, 11, 12, 13, 15], "19": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 13, 14, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15], "21": [0, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15], "23": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15]}, "type_vocab_size": 2, "vocab_size": 30522}, "eval_metrics": {"exact_match": 81.49479659413434, "f1": 88.43605833215561}, "path": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60_d0.25/checkpoint-50000", "source_checkpoint": "/data_2to/devel_data/nn_pruning/output/squad_test_large/large_regu_60_d0.25/checkpoint-221320", "sparse_args": {"ampere_pruning_method": "disabled", "attention_block_cols": 1, "attention_block_rows": 1, "attention_lambda": 1.0, "attention_output_with_dense": 0, "attention_pruning_method": "topK", "bias_mask": true, "dense_block_cols": 1, "dense_block_rows": 1, "dense_lambda": 1.0, "dense_pruning_method": "topK", "distil_alpha_ce": 0.1, "distil_alpha_teacher": 0.9, "distil_teacher_name_or_path": "bert-large-uncased-whole-word-masking-finetuned-squad", "distil_temperature": 2.0, "final_ampere_temperature": 20.0, "final_finetune": 1, "final_threshold": 0.5, "final_warmup": 0, "initial_ampere_temperature": 0.0, "initial_threshold": 1.0, "initial_warmup": 0, "mask_init": "constant", "mask_scale": 0.0, "mask_scores_learning_rate": 0.01, "regularization": "", "regularization_final_lambda": 0}, "speed": {"cuda_eval_elapsed_time": 27.367526008605957, "eval_elapsed_time": 34.362684624269605}, "speedup": 1.4102258637932692, "stats": {"layers": {"0": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 231424, "linear_dense_total": 8388608, "linear_nnz": 1017856, "linear_total": 12582912, "nnz": 1024689, "total": 12593728}, "1": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 290816, "linear_dense_total": 8388608, "linear_nnz": 552960, "linear_total": 12582912, "nnz": 559438, "total": 12593344}, "10": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 923648, "linear_dense_total": 8388608, "linear_nnz": 2496512, "linear_total": 12582912, "nnz": 2504259, "total": 12594304}, "11": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 1075200, "linear_dense_total": 8388608, "linear_nnz": 2385920, "linear_total": 12582912, "nnz": 2393549, "total": 12594112}, "12": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 1126400, "linear_dense_total": 8388608, "linear_nnz": 2174976, "linear_total": 12582912, "nnz": 2182438, "total": 12593920}, "13": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 1179648, "linear_dense_total": 8388608, "linear_nnz": 3276800, "linear_total": 12582912, "nnz": 3285056, "total": 12594688}, "14": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 929792, "linear_dense_total": 8388608, "linear_nnz": 3026944, "linear_total": 12582912, "nnz": 3035078, "total": 12594688}, "15": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 622592, "linear_dense_total": 8388608, "linear_nnz": 2195456, "linear_total": 12582912, "nnz": 2203056, "total": 12594304}, "16": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 403456, "linear_dense_total": 8388608, "linear_nnz": 2500608, "linear_total": 12582912, "nnz": 2508485, "total": 12594688}, "17": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 360448, "linear_dense_total": 8388608, "linear_nnz": 2719744, "linear_total": 12582912, "nnz": 2727792, "total": 12594880}, "18": {"linear_attention_nnz": 1835008, "linear_attention_total": 4194304, "linear_dense_nnz": 335872, "linear_dense_total": 8388608, "linear_nnz": 2170880, "linear_total": 12582912, "nnz": 2178532, "total": 12594496}, "19": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 307200, "linear_dense_total": 8388608, "linear_nnz": 1093632, "linear_total": 12582912, "nnz": 1100502, "total": 12593728}, "2": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 331776, "linear_dense_total": 8388608, "linear_nnz": 856064, "linear_total": 12582912, "nnz": 862754, "total": 12593536}, "20": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 165888, "linear_dense_total": 8388608, "linear_nnz": 690176, "linear_total": 12582912, "nnz": 696785, "total": 12593536}, "21": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 75776, "linear_dense_total": 8388608, "linear_nnz": 862208, "linear_total": 12582912, "nnz": 868965, "total": 12593728}, "22": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 100352, "linear_dense_total": 8388608, "linear_nnz": 624640, "linear_total": 12582912, "nnz": 631217, "total": 12593536}, "23": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 184320, "linear_dense_total": 8388608, "linear_nnz": 446464, "linear_total": 12582912, "nnz": 452890, "total": 12593344}, "3": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 432128, "linear_dense_total": 8388608, "linear_nnz": 956416, "linear_total": 12582912, "nnz": 963155, "total": 12593536}, "4": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 452608, "linear_dense_total": 8388608, "linear_nnz": 976896, "linear_total": 12582912, "nnz": 983645, "total": 12593536}, "5": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 614400, "linear_dense_total": 8388608, "linear_nnz": 876544, "linear_total": 12582912, "nnz": 883180, "total": 12593344}, "6": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 598016, "linear_dense_total": 8388608, "linear_nnz": 860160, "linear_total": 12582912, "nnz": 866788, "total": 12593344}, "7": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 466944, "linear_dense_total": 8388608, "linear_nnz": 1253376, "linear_total": 12582912, "nnz": 1260324, "total": 12593728}, "8": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 673792, "linear_dense_total": 8388608, "linear_nnz": 935936, "linear_total": 12582912, "nnz": 942601, "total": 12593344}, "9": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 692224, "linear_dense_total": 8388608, "linear_nnz": 1478656, "linear_total": 12582912, "nnz": 1485714, "total": 12593728}}, "linear_nnz": 36429824, "linear_sparsity": 87.93674045138889, "linear_total": 301989888, "nnz": 68385854, "pruned_heads": {"0": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 3, 6, 7, 8, 10, 12, 13, 14, 15], "11": [0, 1, 2, 4, 5, 6, 7, 8, 10, 12, 15], "12": [1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14], "13": [2, 3, 4, 9, 10, 11, 12, 13], "14": [1, 2, 3, 4, 8, 9, 11, 12], "15": [0, 1, 2, 5, 6, 7, 8, 9, 11, 12], "16": [3, 6, 7, 8, 10, 12, 13, 15], "17": [0, 2, 4, 8, 11, 12, 15], "18": [2, 3, 5, 9, 10, 11, 12, 13, 15], "19": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 13, 14, 15], "2": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15], "21": [0, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15], "23": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15]}, "total": 334038082, "total_sparsity": 79.52752764279134}, "training_args": {"_n_gpu": -1, "adafactor": false, "adam_beta1": 0.9, "adam_beta2": 0.999, "adam_epsilon": 1e-08, "dataloader_drop_last": false, "dataloader_num_workers": 0, "dataloader_pin_memory": true, "ddp_find_unused_parameters": null, "debug": false, "deepspeed": null, "disable_tqdm": false, "do_eval": 1, "do_predict": false, "do_train": 1, "eval_accumulation_steps": null, "eval_steps": 2500, "evaluation_strategy": "steps", "fp16": false, "fp16_backend": "auto", "fp16_full_eval": false, "fp16_opt_level": "O1", "gradient_accumulation_steps": 1, "greater_is_better": null, "group_by_length": false, "ignore_data_skip": false, "label_names": null, "label_smoothing_factor": 0.0, "learning_rate": 3e-05, "load_best_model_at_end": false, "local_rank": -1, "logging_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60_d0.25", "logging_first_step": false, "logging_steps": 250, "logging_strategy": "steps", "lr_scheduler_type": "linear", "max_grad_norm": 1.0, "max_steps": -1, "metric_for_best_model": null, "no_cuda": false, "num_train_epochs": 10, "optimize_model_before_eval": "disabled", "output_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60_d0.25", "overwrite_output_dir": 1, "past_index": -1, "per_device_eval_batch_size": 128, "per_device_train_batch_size": 16, "per_gpu_eval_batch_size": null, "per_gpu_train_batch_size": null, "prediction_loss_only": false, "remove_unused_columns": true, "report_to": null, "run_name": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60_d0.25", "save_steps": 2500, "save_strategy": "steps", "save_total_limit": 50, "seed": 17, "sharded_ddp": "", "skip_memory_metrics": false, "tpu_metrics_debug": false, "tpu_num_cores": null, "warmup_ratio": 0.0, "warmup_steps": 10, "weight_decay": 0.0}}, "f1": 88.43605833215561, "fill_rate": 0.3216869212962964, "speedup": 1.4102258637932692}}
{"speedup": 1.4644927453324936, "f1": 87.48044078095904, "meta": {"annotate": "9", "cat_fun_name": "is_new_xp", "checkpoint": {"config": {"_name_or_path": "/home/lagunas/devel/hf/nn_pruning/nn_pruning/analysis/tmp_finetune", "architectures": ["BertForQuestionAnswering"], "attention_probs_dropout_prob": 0.1, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-12, "max_position_embeddings": 512, "model_type": "bert", "num_attention_heads": 16, "num_hidden_layers": 24, "pad_token_id": 0, "position_embedding_type": "absolute", "pruned_heads": {"0": [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 3, 6, 7, 8, 9, 10, 12, 13, 14], "11": [0, 1, 2, 4, 5, 6, 8, 10, 12, 15], "12": [1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14], "13": [2, 3, 4, 9, 10, 11, 12], "14": [1, 2, 3, 4, 8, 9, 11, 12], "15": [0, 1, 2, 4, 5, 6, 7, 8, 9, 11, 12], "16": [3, 6, 8, 10, 12, 13, 15], "17": [0, 2, 4, 8, 11, 12, 15], "18": [2, 3, 5, 9, 11, 13], "19": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 13, 14, 15], "2": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [0, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15], "23": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 6, 8, 10, 11, 12, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15]}, "type_vocab_size": 2, "vocab_size": 30522}, "eval_metrics": {"exact_match": 79.81078524124882, "f1": 87.48044078095904}, "path": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60/checkpoint-45000", "source_checkpoint": "/data_2to/devel_data/nn_pruning/output/squad_test_large/large_regu_60/checkpoint-221320", "sparse_args": {"ampere_pruning_method": "disabled", "attention_block_cols": 1, "attention_block_rows": 1, "attention_lambda": 1.0, "attention_output_with_dense": 0, "attention_pruning_method": "topK", "bias_mask": true, "dense_block_cols": 1, "dense_block_rows": 1, "dense_lambda": 1.0, "dense_pruning_method": "topK", "distil_alpha_ce": 0.1, "distil_alpha_teacher": 0.9, "distil_teacher_name_or_path": "bert-large-uncased-whole-word-masking-finetuned-squad", "distil_temperature": 2.0, "final_ampere_temperature": 20.0, "final_finetune": 1, "final_threshold": 0.5, "final_warmup": 0, "initial_ampere_temperature": 0.0, "initial_threshold": 1.0, "initial_warmup": 0, "mask_init": "constant", "mask_scale": 0.0, "mask_scores_learning_rate": 0.01, "regularization": "", "regularization_final_lambda": 0}, "speed": {"cuda_eval_elapsed_time": 26.35342041015625, "eval_elapsed_time": 33.25582229997963}, "speedup": 1.4644927453324936, "stats": {"layers": {"0": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 71680, "linear_dense_total": 8388608, "linear_nnz": 858112, "linear_total": 12582912, "nnz": 864867, "total": 12593728}, "1": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 59392, "linear_dense_total": 8388608, "linear_nnz": 321536, "linear_total": 12582912, "nnz": 327901, "total": 12593344}, "10": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 360448, "linear_dense_total": 8388608, "linear_nnz": 1933312, "linear_total": 12582912, "nnz": 1940784, "total": 12594304}, "11": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 352256, "linear_dense_total": 8388608, "linear_nnz": 1925120, "linear_total": 12582912, "nnz": 1932588, "total": 12594304}, "12": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 460800, "linear_dense_total": 8388608, "linear_nnz": 1509376, "linear_total": 12582912, "nnz": 1516513, "total": 12593920}, "13": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 382976, "linear_dense_total": 8388608, "linear_nnz": 2742272, "linear_total": 12582912, "nnz": 2750331, "total": 12594880}, "14": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 276480, "linear_dense_total": 8388608, "linear_nnz": 2373632, "linear_total": 12582912, "nnz": 2381447, "total": 12594688}, "15": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 258048, "linear_dense_total": 8388608, "linear_nnz": 1568768, "linear_total": 12582912, "nnz": 1575998, "total": 12594112}, "16": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 147456, "linear_dense_total": 8388608, "linear_nnz": 2506752, "linear_total": 12582912, "nnz": 2514696, "total": 12594880}, "17": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 145408, "linear_dense_total": 8388608, "linear_nnz": 2504704, "linear_total": 12582912, "nnz": 2512647, "total": 12594880}, "18": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 116736, "linear_dense_total": 8388608, "linear_nnz": 2738176, "linear_total": 12582912, "nnz": 2746297, "total": 12595072}, "19": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 100352, "linear_dense_total": 8388608, "linear_nnz": 886784, "linear_total": 12582912, "nnz": 893553, "total": 12593728}, "2": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 63488, "linear_dense_total": 8388608, "linear_nnz": 325632, "linear_total": 12582912, "nnz": 331999, "total": 12593344}, "20": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 49152, "linear_dense_total": 8388608, "linear_nnz": 835584, "linear_total": 12582912, "nnz": 842328, "total": 12593728}, "21": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 40960, "linear_dense_total": 8388608, "linear_nnz": 827392, "linear_total": 12582912, "nnz": 834132, "total": 12593728}, "22": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 49152, "linear_dense_total": 8388608, "linear_nnz": 573440, "linear_total": 12582912, "nnz": 579992, "total": 12593536}, "23": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 81920, "linear_dense_total": 8388608, "linear_nnz": 606208, "linear_total": 12582912, "nnz": 612776, "total": 12593536}, "3": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 100352, "linear_dense_total": 8388608, "linear_nnz": 624640, "linear_total": 12582912, "nnz": 631217, "total": 12593536}, "4": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 133120, "linear_dense_total": 8388608, "linear_nnz": 657408, "linear_total": 12582912, "nnz": 664001, "total": 12593536}, "5": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 129024, "linear_dense_total": 8388608, "linear_nnz": 391168, "linear_total": 12582912, "nnz": 397567, "total": 12593344}, "6": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 155648, "linear_dense_total": 8388608, "linear_nnz": 417792, "linear_total": 12582912, "nnz": 424204, "total": 12593344}, "7": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 145408, "linear_dense_total": 8388608, "linear_nnz": 1193984, "linear_total": 12582912, "nnz": 1200967, "total": 12593920}, "8": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 174080, "linear_dense_total": 8388608, "linear_nnz": 436224, "linear_total": 12582912, "nnz": 442645, "total": 12593344}, "9": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 274432, "linear_dense_total": 8388608, "linear_nnz": 1060864, "linear_total": 12582912, "nnz": 1067718, "total": 12593728}}, "linear_nnz": 29818880, "linear_sparsity": 90.12586805555556, "linear_total": 301989888, "nnz": 61772130, "pruned_heads": {"0": [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 3, 6, 7, 8, 9, 10, 12, 13, 14], "11": [0, 1, 2, 4, 5, 6, 8, 10, 12, 15], "12": [1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14], "13": [2, 3, 4, 9, 10, 11, 12], "14": [1, 2, 3, 4, 8, 9, 11, 12], "15": [0, 1, 2, 4, 5, 6, 7, 8, 9, 11, 12], "16": [3, 6, 8, 10, 12, 13, 15], "17": [0, 2, 4, 8, 11, 12, 15], "18": [2, 3, 5, 9, 11, 13], "19": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 13, 14, 15], "2": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [0, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15], "23": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 6, 8, 10, 11, 12, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15]}, "total": 334039426, "total_sparsity": 81.50753318561863}, "training_args": {"_n_gpu": -1, "adafactor": false, "adam_beta1": 0.9, "adam_beta2": 0.999, "adam_epsilon": 1e-08, "dataloader_drop_last": false, "dataloader_num_workers": 0, "dataloader_pin_memory": true, "ddp_find_unused_parameters": null, "debug": false, "deepspeed": null, "disable_tqdm": false, "do_eval": 1, "do_predict": false, "do_train": 1, "eval_accumulation_steps": null, "eval_steps": 2500, "evaluation_strategy": "steps", "fp16": false, "fp16_backend": "auto", "fp16_full_eval": false, "fp16_opt_level": "O1", "gradient_accumulation_steps": 1, "greater_is_better": null, "group_by_length": false, "ignore_data_skip": false, "label_names": null, "label_smoothing_factor": 0.0, "learning_rate": 3e-05, "load_best_model_at_end": false, "local_rank": -1, "logging_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60", "logging_first_step": false, "logging_steps": 250, "logging_strategy": "steps", "lr_scheduler_type": "linear", "max_grad_norm": 1.0, "max_steps": -1, "metric_for_best_model": null, "no_cuda": false, "num_train_epochs": 10, "optimize_model_before_eval": "disabled", "output_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60", "overwrite_output_dir": 1, "past_index": -1, "per_device_eval_batch_size": 128, "per_device_train_batch_size": 16, "per_gpu_eval_batch_size": null, "per_gpu_train_batch_size": null, "prediction_loss_only": false, "remove_unused_columns": true, "report_to": null, "run_name": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60", "save_steps": 2500, "save_strategy": "steps", "save_total_limit": 50, "seed": 17, "sharded_ddp": "", "skip_memory_metrics": false, "tpu_metrics_debug": false, "tpu_num_cores": null, "warmup_ratio": 0.0, "warmup_steps": 10, "weight_decay": 0.0}}, "f1": 87.48044078095904, "fill_rate": 0.2633101851851851, "speedup": 1.4644927453324936}}
{"speedup": 1.4810163272660417, "f1": 87.41551421723396, "meta": {"annotate": "9", "cat_fun_name": "is_new_xp", "checkpoint": {"config": {"_name_or_path": "/home/lagunas/devel/hf/nn_pruning/nn_pruning/analysis/tmp_finetune", "architectures": ["BertForQuestionAnswering"], "attention_probs_dropout_prob": 0.1, "gradient_checkpointing": false, "hidden_act": "gelu", "hidden_dropout_prob": 0.1, "hidden_size": 1024, "initializer_range": 0.02, "intermediate_size": 4096, "layer_norm_eps": 1e-12, "max_position_embeddings": 512, "model_type": "bert", "num_attention_heads": 16, "num_hidden_layers": 24, "pad_token_id": 0, "position_embedding_type": "absolute", "pruned_heads": {"0": [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 3, 6, 7, 8, 9, 10, 12, 13, 14], "11": [0, 1, 2, 4, 5, 6, 8, 10, 12, 15], "12": [1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14], "13": [2, 3, 4, 9, 10, 11, 12], "14": [1, 2, 3, 4, 8, 9, 11, 12], "15": [0, 1, 2, 4, 5, 6, 7, 8, 9, 11, 12], "16": [3, 6, 8, 10, 12, 13, 15], "17": [0, 2, 4, 8, 11, 12, 15], "18": [2, 3, 5, 9, 11, 13], "19": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 13, 14, 15], "2": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [0, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15], "23": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 6, 8, 10, 11, 12, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15]}, "type_vocab_size": 2, "vocab_size": 30522}, "eval_metrics": {"exact_match": 79.84862819299906, "f1": 87.41551421723396}, "path": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60/checkpoint-55000", "source_checkpoint": "/data_2to/devel_data/nn_pruning/output/squad_test_large/large_regu_60/checkpoint-221320", "sparse_args": {"ampere_pruning_method": "disabled", "attention_block_cols": 1, "attention_block_rows": 1, "attention_lambda": 1.0, "attention_output_with_dense": 0, "attention_pruning_method": "topK", "bias_mask": true, "dense_block_cols": 1, "dense_block_rows": 1, "dense_lambda": 1.0, "dense_pruning_method": "topK", "distil_alpha_ce": 0.1, "distil_alpha_teacher": 0.9, "distil_teacher_name_or_path": "bert-large-uncased-whole-word-masking-finetuned-squad", "distil_temperature": 2.0, "final_ampere_temperature": 20.0, "final_finetune": 1, "final_threshold": 0.5, "final_warmup": 0, "initial_ampere_temperature": 0.0, "initial_threshold": 1.0, "initial_warmup": 0, "mask_init": "constant", "mask_scale": 0.0, "mask_scores_learning_rate": 0.01, "regularization": "", "regularization_final_lambda": 0}, "speed": {"cuda_eval_elapsed_time": 26.059397384643557, "eval_elapsed_time": 33.108247430063784}, "speedup": 1.4810163272660417, "stats": {"layers": {"0": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 71680, "linear_dense_total": 8388608, "linear_nnz": 858112, "linear_total": 12582912, "nnz": 864867, "total": 12593728}, "1": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 59392, "linear_dense_total": 8388608, "linear_nnz": 321536, "linear_total": 12582912, "nnz": 327901, "total": 12593344}, "10": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 360448, "linear_dense_total": 8388608, "linear_nnz": 1933312, "linear_total": 12582912, "nnz": 1940784, "total": 12594304}, "11": {"linear_attention_nnz": 1572864, "linear_attention_total": 4194304, "linear_dense_nnz": 352256, "linear_dense_total": 8388608, "linear_nnz": 1925120, "linear_total": 12582912, "nnz": 1932588, "total": 12594304}, "12": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 460800, "linear_dense_total": 8388608, "linear_nnz": 1509376, "linear_total": 12582912, "nnz": 1516513, "total": 12593920}, "13": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 382976, "linear_dense_total": 8388608, "linear_nnz": 2742272, "linear_total": 12582912, "nnz": 2750331, "total": 12594880}, "14": {"linear_attention_nnz": 2097152, "linear_attention_total": 4194304, "linear_dense_nnz": 276480, "linear_dense_total": 8388608, "linear_nnz": 2373632, "linear_total": 12582912, "nnz": 2381447, "total": 12594688}, "15": {"linear_attention_nnz": 1310720, "linear_attention_total": 4194304, "linear_dense_nnz": 258048, "linear_dense_total": 8388608, "linear_nnz": 1568768, "linear_total": 12582912, "nnz": 1575998, "total": 12594112}, "16": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 147456, "linear_dense_total": 8388608, "linear_nnz": 2506752, "linear_total": 12582912, "nnz": 2514696, "total": 12594880}, "17": {"linear_attention_nnz": 2359296, "linear_attention_total": 4194304, "linear_dense_nnz": 145408, "linear_dense_total": 8388608, "linear_nnz": 2504704, "linear_total": 12582912, "nnz": 2512647, "total": 12594880}, "18": {"linear_attention_nnz": 2621440, "linear_attention_total": 4194304, "linear_dense_nnz": 116736, "linear_dense_total": 8388608, "linear_nnz": 2738176, "linear_total": 12582912, "nnz": 2746297, "total": 12595072}, "19": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 100352, "linear_dense_total": 8388608, "linear_nnz": 886784, "linear_total": 12582912, "nnz": 893553, "total": 12593728}, "2": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 63488, "linear_dense_total": 8388608, "linear_nnz": 325632, "linear_total": 12582912, "nnz": 331999, "total": 12593344}, "20": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 49152, "linear_dense_total": 8388608, "linear_nnz": 835584, "linear_total": 12582912, "nnz": 842328, "total": 12593728}, "21": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 40960, "linear_dense_total": 8388608, "linear_nnz": 827392, "linear_total": 12582912, "nnz": 834132, "total": 12593728}, "22": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 49152, "linear_dense_total": 8388608, "linear_nnz": 573440, "linear_total": 12582912, "nnz": 579992, "total": 12593536}, "23": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 81920, "linear_dense_total": 8388608, "linear_nnz": 606208, "linear_total": 12582912, "nnz": 612776, "total": 12593536}, "3": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 100352, "linear_dense_total": 8388608, "linear_nnz": 624640, "linear_total": 12582912, "nnz": 631217, "total": 12593536}, "4": {"linear_attention_nnz": 524288, "linear_attention_total": 4194304, "linear_dense_nnz": 133120, "linear_dense_total": 8388608, "linear_nnz": 657408, "linear_total": 12582912, "nnz": 664001, "total": 12593536}, "5": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 129024, "linear_dense_total": 8388608, "linear_nnz": 391168, "linear_total": 12582912, "nnz": 397567, "total": 12593344}, "6": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 155648, "linear_dense_total": 8388608, "linear_nnz": 417792, "linear_total": 12582912, "nnz": 424204, "total": 12593344}, "7": {"linear_attention_nnz": 1048576, "linear_attention_total": 4194304, "linear_dense_nnz": 145408, "linear_dense_total": 8388608, "linear_nnz": 1193984, "linear_total": 12582912, "nnz": 1200967, "total": 12593920}, "8": {"linear_attention_nnz": 262144, "linear_attention_total": 4194304, "linear_dense_nnz": 174080, "linear_dense_total": 8388608, "linear_nnz": 436224, "linear_total": 12582912, "nnz": 442645, "total": 12593344}, "9": {"linear_attention_nnz": 786432, "linear_attention_total": 4194304, "linear_dense_nnz": 274432, "linear_dense_total": 8388608, "linear_nnz": 1060864, "linear_total": 12582912, "nnz": 1067718, "total": 12593728}}, "linear_nnz": 29818880, "linear_sparsity": 90.12586805555556, "linear_total": 301989888, "nnz": 61772130, "pruned_heads": {"0": [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15], "1": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "10": [0, 3, 6, 7, 8, 9, 10, 12, 13, 14], "11": [0, 1, 2, 4, 5, 6, 8, 10, 12, 15], "12": [1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14], "13": [2, 3, 4, 9, 10, 11, 12], "14": [1, 2, 3, 4, 8, 9, 11, 12], "15": [0, 1, 2, 4, 5, 6, 7, 8, 9, 11, 12], "16": [3, 6, 8, 10, 12, 13, 15], "17": [0, 2, 4, 8, 11, 12, 15], "18": [2, 3, 5, 9, 11, 13], "19": [0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 13, 14, 15], "2": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15], "20": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15], "21": [0, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15], "22": [0, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15], "23": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14], "3": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15], "4": [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "5": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "6": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15], "7": [0, 1, 2, 4, 6, 8, 10, 11, 12, 13, 14, 15], "8": [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15], "9": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15]}, "total": 334039426, "total_sparsity": 81.50753318561863}, "training_args": {"_n_gpu": -1, "adafactor": false, "adam_beta1": 0.9, "adam_beta2": 0.999, "adam_epsilon": 1e-08, "dataloader_drop_last": false, "dataloader_num_workers": 0, "dataloader_pin_memory": true, "ddp_find_unused_parameters": null, "debug": false, "deepspeed": null, "disable_tqdm": false, "do_eval": 1, "do_predict": false, "do_train": 1, "eval_accumulation_steps": null, "eval_steps": 2500, "evaluation_strategy": "steps", "fp16": false, "fp16_backend": "auto", "fp16_full_eval": false, "fp16_opt_level": "O1", "gradient_accumulation_steps": 1, "greater_is_better": null, "group_by_length": false, "ignore_data_skip": false, "label_names": null, "label_smoothing_factor": 0.0, "learning_rate": 3e-05, "load_best_model_at_end": false, "local_rank": -1, "logging_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60", "logging_first_step": false, "logging_steps": 250, "logging_strategy": "steps", "lr_scheduler_type": "linear", "max_grad_norm": 1.0, "max_steps": -1, "metric_for_best_model": null, "no_cuda": false, "num_train_epochs": 10, "optimize_model_before_eval": "disabled", "output_dir": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60", "overwrite_output_dir": 1, "past_index": -1, "per_device_eval_batch_size": 128, "per_device_train_batch_size": 16, "per_gpu_eval_batch_size": null, "per_gpu_train_batch_size": null, "prediction_loss_only": false, "remove_unused_columns": true, "report_to": null, "run_name": "/data_2to/devel_data/nn_pruning/output/squad_test_final_fine_tune/fine_tuned_large_regu_60", "save_steps": 2500, "save_strategy": "steps", "save_total_limit": 50, "seed": 17, "sharded_ddp": "", "skip_memory_metrics": false, "tpu_metrics_debug": false, "tpu_num_cores": null, "warmup_ratio": 0.0, "warmup_steps": 10, "weight_decay": 0.0}}, "f1": 87.41551421723396, "fill_rate": 0.2633101851851851, "speedup": 1.4810163272660417}}
