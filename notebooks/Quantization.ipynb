{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b484300",
   "metadata": {},
   "source": [
    "## Graph Mode Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586859cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "from nn_pruning.inference_model_patcher import optimize_model\n",
    "from nn_pruning.modules.quantization import prepare_static, quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f7b99",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f655b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 1\n",
    "SEQLEN = 384\n",
    "DOC_STRIDE= 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7d11f",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf7e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"madlag/bert-base-uncased-squadv1-x2.01-f89.2-d30-hybrid-rewind-opt-v1\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name, torchscript=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83924325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed heads 0, total_heads=89, percentage removed=0.0\n",
      "bert.encoder.layer.0.intermediate.dense, sparsity = 84.44\n",
      "bert.encoder.layer.0.output.dense, sparsity = 84.44\n",
      "bert.encoder.layer.1.intermediate.dense, sparsity = 82.75\n",
      "bert.encoder.layer.1.output.dense, sparsity = 82.75\n",
      "bert.encoder.layer.2.intermediate.dense, sparsity = 78.35\n",
      "bert.encoder.layer.2.output.dense, sparsity = 78.35\n",
      "bert.encoder.layer.3.intermediate.dense, sparsity = 79.56\n",
      "bert.encoder.layer.3.output.dense, sparsity = 79.56\n",
      "bert.encoder.layer.4.intermediate.dense, sparsity = 82.29\n",
      "bert.encoder.layer.4.output.dense, sparsity = 82.29\n",
      "bert.encoder.layer.5.intermediate.dense, sparsity = 81.84\n",
      "bert.encoder.layer.5.output.dense, sparsity = 81.84\n",
      "bert.encoder.layer.6.intermediate.dense, sparsity = 84.31\n",
      "bert.encoder.layer.6.output.dense, sparsity = 84.31\n",
      "bert.encoder.layer.7.intermediate.dense, sparsity = 88.05\n",
      "bert.encoder.layer.7.output.dense, sparsity = 88.05\n",
      "bert.encoder.layer.8.intermediate.dense, sparsity = 94.01\n",
      "bert.encoder.layer.8.output.dense, sparsity = 94.01\n",
      "bert.encoder.layer.9.intermediate.dense, sparsity = 95.74\n",
      "bert.encoder.layer.9.output.dense, sparsity = 95.74\n",
      "bert.encoder.layer.10.intermediate.dense, sparsity = 89.29\n",
      "bert.encoder.layer.10.output.dense, sparsity = 89.29\n",
      "bert.encoder.layer.11.intermediate.dense, sparsity = 84.57\n",
      "bert.encoder.layer.11.output.dense, sparsity = 84.57\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(model, mode=\"dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11dfe139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/miniconda3/envs/nn_pruning/lib/python3.8/site-packages/torch/quantization/observer.py:121: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prepared_model = prepare_static(\n",
    "    optimized_model,\n",
    "    input_names=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
    "    batch_size=BS,\n",
    "    sequence_length=SEQLEN,\n",
    "    qconfig_name=\"default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a75902d",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3199b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_example(examples, tokenizer, max_length=SEQLEN, doc_stride=DOC_STRIDE):\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03bb7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e176f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/michael/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963b0d258f024be794d8bcd74b761528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nb_representative_samples = 200\n",
    "representative_dataset = load_dataset('squad', split=f'train[:{nb_representative_samples}]')\n",
    "representative_dataset = representative_dataset.map(\n",
    "    functools.partial(prepare_example, tokenizer=tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=representative_dataset.column_names\n",
    ")\n",
    "representative_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids'])\n",
    "dataloader = torch.utils.data.DataLoader(representative_dataset, batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94766b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:58<00:00,  3.41it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for examples in tqdm(dataloader):\n",
    "        prepared_model(**examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29af7c2",
   "metadata": {},
   "source": [
    "### Conversion and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c649686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8 = quantize(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4475080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that inference is working properly.\n",
    "model_int8_output = model_int8(**model_int8.dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "297fe1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<eval_with_key_9>:9: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_1 = torch.quantize_per_tensor(sub_1, _input_scale_0, _input_zero_point_0, _input_dtype_0);  sub_1 = _input_scale_0 = _input_zero_point_0 = _input_dtype_0 = None\n",
      "<eval_with_key_9>:15: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_2 = torch.quantize_per_tensor(bert_embeddings_word_embeddings, _input_scale_1, _input_zero_point_1, _input_dtype_1);  bert_embeddings_word_embeddings = _input_scale_1 = _input_zero_point_1 = _input_dtype_1 = None\n",
      "<eval_with_key_9>:20: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_3 = torch.quantize_per_tensor(bert_embeddings_token_type_embeddings, _input_scale_2, _input_zero_point_2, _input_dtype_2);  bert_embeddings_token_type_embeddings = _input_scale_2 = _input_zero_point_2 = _input_dtype_2 = None\n",
      "<eval_with_key_9>:23: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_1 = torch.ops.quantized.add(quantize_per_tensor_2, quantize_per_tensor_3, _scale_0, _zero_point_0);  quantize_per_tensor_2 = quantize_per_tensor_3 = _scale_0 = _zero_point_0 = None\n",
      "<eval_with_key_9>:29: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_4 = torch.quantize_per_tensor(bert_embeddings_position_embeddings, _input_scale_3, _input_zero_point_3, _input_dtype_3);  bert_embeddings_position_embeddings = _input_scale_3 = _input_zero_point_3 = _input_dtype_3 = None\n",
      "<eval_with_key_9>:32: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_2 = torch.ops.quantized.add(add_1, quantize_per_tensor_4, _scale_1, _zero_point_1);  add_1 = quantize_per_tensor_4 = _scale_1 = _zero_point_1 = None\n",
      "<eval_with_key_9>:37: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_5 = torch.quantize_per_tensor(bert_embeddings_layer_norm_weight, _input_scale_4, _input_zero_point_4, _input_dtype_4);  bert_embeddings_layer_norm_weight = _input_scale_4 = _input_zero_point_4 = _input_dtype_4 = None\n",
      "<eval_with_key_9>:40: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_2 = torch.ops.quantized.mul(add_2, quantize_per_tensor_5, _scale_2, _zero_point_2);  add_2 = quantize_per_tensor_5 = _scale_2 = _zero_point_2 = None\n",
      "<eval_with_key_9>:45: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_6 = torch.quantize_per_tensor(bert_embeddings_layer_norm_bias, _input_scale_5, _input_zero_point_5, _input_dtype_5);  bert_embeddings_layer_norm_bias = _input_scale_5 = _input_zero_point_5 = _input_dtype_5 = None\n",
      "<eval_with_key_9>:50: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_3 = torch.ops.quantized.add(mul_2, broadcast_to_13, _scale_3, _zero_point_3);  mul_2 = broadcast_to_13 = _scale_3 = _zero_point_3 = None\n",
      "<eval_with_key_9>:69: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_7 = torch.quantize_per_tensor(truediv, _input_scale_6, _input_zero_point_6, _input_dtype_6);  truediv = _input_scale_6 = _input_zero_point_6 = _input_dtype_6 = None\n",
      "<eval_with_key_9>:74: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_4 = torch.ops.quantized.add(quantize_per_tensor_7, broadcast_to_1, _scale_4, _zero_point_4);  quantize_per_tensor_7 = broadcast_to_1 = _scale_4 = _zero_point_4 = None\n",
      "<eval_with_key_9>:86: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_8 = torch.quantize_per_tensor(view_3, bert_encoder_layer_0_attention_output_dense_input_scale_0, bert_encoder_layer_0_attention_output_dense_input_zero_point_0, bert_encoder_layer_0_attention_output_dense_input_dtype_0);  view_3 = bert_encoder_layer_0_attention_output_dense_input_scale_0 = bert_encoder_layer_0_attention_output_dense_input_zero_point_0 = bert_encoder_layer_0_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:91: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_5 = torch.ops.quantized.add(bert_encoder_layer_0_attention_output_dropout, bert_embeddings_dropout, _scale_5, _zero_point_5);  bert_encoder_layer_0_attention_output_dropout = bert_embeddings_dropout = _scale_5 = _zero_point_5 = None\n",
      "<eval_with_key_9>:96: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_9 = torch.quantize_per_tensor(bert_encoder_layer_0_attention_output_layer_norm_weight, _input_scale_7, _input_zero_point_7, _input_dtype_7);  bert_encoder_layer_0_attention_output_layer_norm_weight = _input_scale_7 = _input_zero_point_7 = _input_dtype_7 = None\n",
      "<eval_with_key_9>:99: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_3 = torch.ops.quantized.mul(add_5, quantize_per_tensor_9, _scale_6, _zero_point_6);  add_5 = quantize_per_tensor_9 = _scale_6 = _zero_point_6 = None\n",
      "<eval_with_key_9>:104: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_10 = torch.quantize_per_tensor(bert_encoder_layer_0_attention_output_layer_norm_bias, _input_scale_8, _input_zero_point_8, _input_dtype_8);  bert_encoder_layer_0_attention_output_layer_norm_bias = _input_scale_8 = _input_zero_point_8 = _input_dtype_8 = None\n",
      "<eval_with_key_9>:109: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_6 = torch.ops.quantized.add(mul_3, broadcast_to_14, _scale_7, _zero_point_7);  mul_3 = broadcast_to_14 = _scale_7 = _zero_point_7 = None\n",
      "<eval_with_key_9>:115: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_7 = torch.ops.quantized.add(bert_encoder_layer_0_output_dropout, add_6, _scale_8, _zero_point_8);  bert_encoder_layer_0_output_dropout = add_6 = _scale_8 = _zero_point_8 = None\n",
      "<eval_with_key_9>:120: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_11 = torch.quantize_per_tensor(bert_encoder_layer_0_output_layer_norm_weight, _input_scale_9, _input_zero_point_9, _input_dtype_9);  bert_encoder_layer_0_output_layer_norm_weight = _input_scale_9 = _input_zero_point_9 = _input_dtype_9 = None\n",
      "<eval_with_key_9>:123: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_4 = torch.ops.quantized.mul(add_7, quantize_per_tensor_11, _scale_9, _zero_point_9);  add_7 = quantize_per_tensor_11 = _scale_9 = _zero_point_9 = None\n",
      "<eval_with_key_9>:128: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_12 = torch.quantize_per_tensor(bert_encoder_layer_0_output_layer_norm_bias, _input_scale_10, _input_zero_point_10, _input_dtype_10);  bert_encoder_layer_0_output_layer_norm_bias = _input_scale_10 = _input_zero_point_10 = _input_dtype_10 = None\n",
      "<eval_with_key_9>:133: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_8 = torch.ops.quantized.add(mul_4, broadcast_to_15, _scale_10, _zero_point_10);  mul_4 = broadcast_to_15 = _scale_10 = _zero_point_10 = None\n",
      "<eval_with_key_9>:151: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_13 = torch.quantize_per_tensor(truediv_1, _input_scale_11, _input_zero_point_11, _input_dtype_11);  truediv_1 = _input_scale_11 = _input_zero_point_11 = _input_dtype_11 = None\n",
      "<eval_with_key_9>:156: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_9 = torch.ops.quantized.add(quantize_per_tensor_13, broadcast_to_2, _scale_11, _zero_point_11);  quantize_per_tensor_13 = broadcast_to_2 = _scale_11 = _zero_point_11 = None\n",
      "<eval_with_key_9>:168: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_14 = torch.quantize_per_tensor(view_7, bert_encoder_layer_1_attention_output_dense_input_scale_0, bert_encoder_layer_1_attention_output_dense_input_zero_point_0, bert_encoder_layer_1_attention_output_dense_input_dtype_0);  view_7 = bert_encoder_layer_1_attention_output_dense_input_scale_0 = bert_encoder_layer_1_attention_output_dense_input_zero_point_0 = bert_encoder_layer_1_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:173: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_10 = torch.ops.quantized.add(bert_encoder_layer_1_attention_output_dropout, add_8, _scale_12, _zero_point_12);  bert_encoder_layer_1_attention_output_dropout = add_8 = _scale_12 = _zero_point_12 = None\n",
      "<eval_with_key_9>:178: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_15 = torch.quantize_per_tensor(bert_encoder_layer_1_attention_output_layer_norm_weight, _input_scale_12, _input_zero_point_12, _input_dtype_12);  bert_encoder_layer_1_attention_output_layer_norm_weight = _input_scale_12 = _input_zero_point_12 = _input_dtype_12 = None\n",
      "<eval_with_key_9>:181: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_5 = torch.ops.quantized.mul(add_10, quantize_per_tensor_15, _scale_13, _zero_point_13);  add_10 = quantize_per_tensor_15 = _scale_13 = _zero_point_13 = None\n",
      "<eval_with_key_9>:186: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_16 = torch.quantize_per_tensor(bert_encoder_layer_1_attention_output_layer_norm_bias, _input_scale_13, _input_zero_point_13, _input_dtype_13);  bert_encoder_layer_1_attention_output_layer_norm_bias = _input_scale_13 = _input_zero_point_13 = _input_dtype_13 = None\n",
      "<eval_with_key_9>:191: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_11 = torch.ops.quantized.add(mul_5, broadcast_to_16, _scale_14, _zero_point_14);  mul_5 = broadcast_to_16 = _scale_14 = _zero_point_14 = None\n",
      "<eval_with_key_9>:197: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_12 = torch.ops.quantized.add(bert_encoder_layer_1_output_dropout, add_11, _scale_15, _zero_point_15);  bert_encoder_layer_1_output_dropout = add_11 = _scale_15 = _zero_point_15 = None\n",
      "<eval_with_key_9>:202: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_17 = torch.quantize_per_tensor(bert_encoder_layer_1_output_layer_norm_weight, _input_scale_14, _input_zero_point_14, _input_dtype_14);  bert_encoder_layer_1_output_layer_norm_weight = _input_scale_14 = _input_zero_point_14 = _input_dtype_14 = None\n",
      "<eval_with_key_9>:205: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_6 = torch.ops.quantized.mul(add_12, quantize_per_tensor_17, _scale_16, _zero_point_16);  add_12 = quantize_per_tensor_17 = _scale_16 = _zero_point_16 = None\n",
      "<eval_with_key_9>:210: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_18 = torch.quantize_per_tensor(bert_encoder_layer_1_output_layer_norm_bias, _input_scale_15, _input_zero_point_15, _input_dtype_15);  bert_encoder_layer_1_output_layer_norm_bias = _input_scale_15 = _input_zero_point_15 = _input_dtype_15 = None\n",
      "<eval_with_key_9>:215: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_13 = torch.ops.quantized.add(mul_6, broadcast_to_17, _scale_17, _zero_point_17);  mul_6 = broadcast_to_17 = _scale_17 = _zero_point_17 = None\n",
      "<eval_with_key_9>:233: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_19 = torch.quantize_per_tensor(truediv_2, _input_scale_16, _input_zero_point_16, _input_dtype_16);  truediv_2 = _input_scale_16 = _input_zero_point_16 = _input_dtype_16 = None\n",
      "<eval_with_key_9>:238: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_14 = torch.ops.quantized.add(quantize_per_tensor_19, broadcast_to_3, _scale_18, _zero_point_18);  quantize_per_tensor_19 = broadcast_to_3 = _scale_18 = _zero_point_18 = None\n",
      "<eval_with_key_9>:250: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_20 = torch.quantize_per_tensor(view_11, bert_encoder_layer_2_attention_output_dense_input_scale_0, bert_encoder_layer_2_attention_output_dense_input_zero_point_0, bert_encoder_layer_2_attention_output_dense_input_dtype_0);  view_11 = bert_encoder_layer_2_attention_output_dense_input_scale_0 = bert_encoder_layer_2_attention_output_dense_input_zero_point_0 = bert_encoder_layer_2_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:255: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_15 = torch.ops.quantized.add(bert_encoder_layer_2_attention_output_dropout, add_13, _scale_19, _zero_point_19);  bert_encoder_layer_2_attention_output_dropout = add_13 = _scale_19 = _zero_point_19 = None\n",
      "<eval_with_key_9>:260: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_21 = torch.quantize_per_tensor(bert_encoder_layer_2_attention_output_layer_norm_weight, _input_scale_17, _input_zero_point_17, _input_dtype_17);  bert_encoder_layer_2_attention_output_layer_norm_weight = _input_scale_17 = _input_zero_point_17 = _input_dtype_17 = None\n",
      "<eval_with_key_9>:263: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_7 = torch.ops.quantized.mul(add_15, quantize_per_tensor_21, _scale_20, _zero_point_20);  add_15 = quantize_per_tensor_21 = _scale_20 = _zero_point_20 = None\n",
      "<eval_with_key_9>:268: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_22 = torch.quantize_per_tensor(bert_encoder_layer_2_attention_output_layer_norm_bias, _input_scale_18, _input_zero_point_18, _input_dtype_18);  bert_encoder_layer_2_attention_output_layer_norm_bias = _input_scale_18 = _input_zero_point_18 = _input_dtype_18 = None\n",
      "<eval_with_key_9>:273: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_16 = torch.ops.quantized.add(mul_7, broadcast_to_18, _scale_21, _zero_point_21);  mul_7 = broadcast_to_18 = _scale_21 = _zero_point_21 = None\n",
      "<eval_with_key_9>:279: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_17 = torch.ops.quantized.add(bert_encoder_layer_2_output_dropout, add_16, _scale_22, _zero_point_22);  bert_encoder_layer_2_output_dropout = add_16 = _scale_22 = _zero_point_22 = None\n",
      "<eval_with_key_9>:284: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_23 = torch.quantize_per_tensor(bert_encoder_layer_2_output_layer_norm_weight, _input_scale_19, _input_zero_point_19, _input_dtype_19);  bert_encoder_layer_2_output_layer_norm_weight = _input_scale_19 = _input_zero_point_19 = _input_dtype_19 = None\n",
      "<eval_with_key_9>:287: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_8 = torch.ops.quantized.mul(add_17, quantize_per_tensor_23, _scale_23, _zero_point_23);  add_17 = quantize_per_tensor_23 = _scale_23 = _zero_point_23 = None\n",
      "<eval_with_key_9>:292: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_24 = torch.quantize_per_tensor(bert_encoder_layer_2_output_layer_norm_bias, _input_scale_20, _input_zero_point_20, _input_dtype_20);  bert_encoder_layer_2_output_layer_norm_bias = _input_scale_20 = _input_zero_point_20 = _input_dtype_20 = None\n",
      "<eval_with_key_9>:297: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_18 = torch.ops.quantized.add(mul_8, broadcast_to_19, _scale_24, _zero_point_24);  mul_8 = broadcast_to_19 = _scale_24 = _zero_point_24 = None\n",
      "<eval_with_key_9>:315: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_25 = torch.quantize_per_tensor(truediv_3, _input_scale_21, _input_zero_point_21, _input_dtype_21);  truediv_3 = _input_scale_21 = _input_zero_point_21 = _input_dtype_21 = None\n",
      "<eval_with_key_9>:320: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_19 = torch.ops.quantized.add(quantize_per_tensor_25, broadcast_to_4, _scale_25, _zero_point_25);  quantize_per_tensor_25 = broadcast_to_4 = _scale_25 = _zero_point_25 = None\n",
      "<eval_with_key_9>:332: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_26 = torch.quantize_per_tensor(view_15, bert_encoder_layer_3_attention_output_dense_input_scale_0, bert_encoder_layer_3_attention_output_dense_input_zero_point_0, bert_encoder_layer_3_attention_output_dense_input_dtype_0);  view_15 = bert_encoder_layer_3_attention_output_dense_input_scale_0 = bert_encoder_layer_3_attention_output_dense_input_zero_point_0 = bert_encoder_layer_3_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:337: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_20 = torch.ops.quantized.add(bert_encoder_layer_3_attention_output_dropout, add_18, _scale_26, _zero_point_26);  bert_encoder_layer_3_attention_output_dropout = add_18 = _scale_26 = _zero_point_26 = None\n",
      "<eval_with_key_9>:342: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_27 = torch.quantize_per_tensor(bert_encoder_layer_3_attention_output_layer_norm_weight, _input_scale_22, _input_zero_point_22, _input_dtype_22);  bert_encoder_layer_3_attention_output_layer_norm_weight = _input_scale_22 = _input_zero_point_22 = _input_dtype_22 = None\n",
      "<eval_with_key_9>:345: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_9 = torch.ops.quantized.mul(add_20, quantize_per_tensor_27, _scale_27, _zero_point_27);  add_20 = quantize_per_tensor_27 = _scale_27 = _zero_point_27 = None\n",
      "<eval_with_key_9>:350: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_28 = torch.quantize_per_tensor(bert_encoder_layer_3_attention_output_layer_norm_bias, _input_scale_23, _input_zero_point_23, _input_dtype_23);  bert_encoder_layer_3_attention_output_layer_norm_bias = _input_scale_23 = _input_zero_point_23 = _input_dtype_23 = None\n",
      "<eval_with_key_9>:355: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_21 = torch.ops.quantized.add(mul_9, broadcast_to_20, _scale_28, _zero_point_28);  mul_9 = broadcast_to_20 = _scale_28 = _zero_point_28 = None\n",
      "<eval_with_key_9>:361: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_22 = torch.ops.quantized.add(bert_encoder_layer_3_output_dropout, add_21, _scale_29, _zero_point_29);  bert_encoder_layer_3_output_dropout = add_21 = _scale_29 = _zero_point_29 = None\n",
      "<eval_with_key_9>:366: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_29 = torch.quantize_per_tensor(bert_encoder_layer_3_output_layer_norm_weight, _input_scale_24, _input_zero_point_24, _input_dtype_24);  bert_encoder_layer_3_output_layer_norm_weight = _input_scale_24 = _input_zero_point_24 = _input_dtype_24 = None\n",
      "<eval_with_key_9>:369: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_10 = torch.ops.quantized.mul(add_22, quantize_per_tensor_29, _scale_30, _zero_point_30);  add_22 = quantize_per_tensor_29 = _scale_30 = _zero_point_30 = None\n",
      "<eval_with_key_9>:374: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_30 = torch.quantize_per_tensor(bert_encoder_layer_3_output_layer_norm_bias, _input_scale_25, _input_zero_point_25, _input_dtype_25);  bert_encoder_layer_3_output_layer_norm_bias = _input_scale_25 = _input_zero_point_25 = _input_dtype_25 = None\n",
      "<eval_with_key_9>:379: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_23 = torch.ops.quantized.add(mul_10, broadcast_to_21, _scale_31, _zero_point_31);  mul_10 = broadcast_to_21 = _scale_31 = _zero_point_31 = None\n",
      "<eval_with_key_9>:397: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_31 = torch.quantize_per_tensor(truediv_4, _input_scale_26, _input_zero_point_26, _input_dtype_26);  truediv_4 = _input_scale_26 = _input_zero_point_26 = _input_dtype_26 = None\n",
      "<eval_with_key_9>:402: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_24 = torch.ops.quantized.add(quantize_per_tensor_31, broadcast_to_5, _scale_32, _zero_point_32);  quantize_per_tensor_31 = broadcast_to_5 = _scale_32 = _zero_point_32 = None\n",
      "<eval_with_key_9>:414: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_32 = torch.quantize_per_tensor(view_19, bert_encoder_layer_4_attention_output_dense_input_scale_0, bert_encoder_layer_4_attention_output_dense_input_zero_point_0, bert_encoder_layer_4_attention_output_dense_input_dtype_0);  view_19 = bert_encoder_layer_4_attention_output_dense_input_scale_0 = bert_encoder_layer_4_attention_output_dense_input_zero_point_0 = bert_encoder_layer_4_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:419: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_25 = torch.ops.quantized.add(bert_encoder_layer_4_attention_output_dropout, add_23, _scale_33, _zero_point_33);  bert_encoder_layer_4_attention_output_dropout = add_23 = _scale_33 = _zero_point_33 = None\n",
      "<eval_with_key_9>:424: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_33 = torch.quantize_per_tensor(bert_encoder_layer_4_attention_output_layer_norm_weight, _input_scale_27, _input_zero_point_27, _input_dtype_27);  bert_encoder_layer_4_attention_output_layer_norm_weight = _input_scale_27 = _input_zero_point_27 = _input_dtype_27 = None\n",
      "<eval_with_key_9>:427: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_11 = torch.ops.quantized.mul(add_25, quantize_per_tensor_33, _scale_34, _zero_point_34);  add_25 = quantize_per_tensor_33 = _scale_34 = _zero_point_34 = None\n",
      "<eval_with_key_9>:432: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_34 = torch.quantize_per_tensor(bert_encoder_layer_4_attention_output_layer_norm_bias, _input_scale_28, _input_zero_point_28, _input_dtype_28);  bert_encoder_layer_4_attention_output_layer_norm_bias = _input_scale_28 = _input_zero_point_28 = _input_dtype_28 = None\n",
      "<eval_with_key_9>:437: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_26 = torch.ops.quantized.add(mul_11, broadcast_to_22, _scale_35, _zero_point_35);  mul_11 = broadcast_to_22 = _scale_35 = _zero_point_35 = None\n",
      "<eval_with_key_9>:443: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_27 = torch.ops.quantized.add(bert_encoder_layer_4_output_dropout, add_26, _scale_36, _zero_point_36);  bert_encoder_layer_4_output_dropout = add_26 = _scale_36 = _zero_point_36 = None\n",
      "<eval_with_key_9>:448: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_35 = torch.quantize_per_tensor(bert_encoder_layer_4_output_layer_norm_weight, _input_scale_29, _input_zero_point_29, _input_dtype_29);  bert_encoder_layer_4_output_layer_norm_weight = _input_scale_29 = _input_zero_point_29 = _input_dtype_29 = None\n",
      "<eval_with_key_9>:451: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_12 = torch.ops.quantized.mul(add_27, quantize_per_tensor_35, _scale_37, _zero_point_37);  add_27 = quantize_per_tensor_35 = _scale_37 = _zero_point_37 = None\n",
      "<eval_with_key_9>:456: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_36 = torch.quantize_per_tensor(bert_encoder_layer_4_output_layer_norm_bias, _input_scale_30, _input_zero_point_30, _input_dtype_30);  bert_encoder_layer_4_output_layer_norm_bias = _input_scale_30 = _input_zero_point_30 = _input_dtype_30 = None\n",
      "<eval_with_key_9>:461: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_28 = torch.ops.quantized.add(mul_12, broadcast_to_23, _scale_38, _zero_point_38);  mul_12 = broadcast_to_23 = _scale_38 = _zero_point_38 = None\n",
      "<eval_with_key_9>:479: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_37 = torch.quantize_per_tensor(truediv_5, _input_scale_31, _input_zero_point_31, _input_dtype_31);  truediv_5 = _input_scale_31 = _input_zero_point_31 = _input_dtype_31 = None\n",
      "<eval_with_key_9>:484: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_29 = torch.ops.quantized.add(quantize_per_tensor_37, broadcast_to_6, _scale_39, _zero_point_39);  quantize_per_tensor_37 = broadcast_to_6 = _scale_39 = _zero_point_39 = None\n",
      "<eval_with_key_9>:496: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_38 = torch.quantize_per_tensor(view_23, bert_encoder_layer_5_attention_output_dense_input_scale_0, bert_encoder_layer_5_attention_output_dense_input_zero_point_0, bert_encoder_layer_5_attention_output_dense_input_dtype_0);  view_23 = bert_encoder_layer_5_attention_output_dense_input_scale_0 = bert_encoder_layer_5_attention_output_dense_input_zero_point_0 = bert_encoder_layer_5_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:501: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_30 = torch.ops.quantized.add(bert_encoder_layer_5_attention_output_dropout, add_28, _scale_40, _zero_point_40);  bert_encoder_layer_5_attention_output_dropout = add_28 = _scale_40 = _zero_point_40 = None\n",
      "<eval_with_key_9>:506: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_39 = torch.quantize_per_tensor(bert_encoder_layer_5_attention_output_layer_norm_weight, _input_scale_32, _input_zero_point_32, _input_dtype_32);  bert_encoder_layer_5_attention_output_layer_norm_weight = _input_scale_32 = _input_zero_point_32 = _input_dtype_32 = None\n",
      "<eval_with_key_9>:509: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_13 = torch.ops.quantized.mul(add_30, quantize_per_tensor_39, _scale_41, _zero_point_41);  add_30 = quantize_per_tensor_39 = _scale_41 = _zero_point_41 = None\n",
      "<eval_with_key_9>:514: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_40 = torch.quantize_per_tensor(bert_encoder_layer_5_attention_output_layer_norm_bias, _input_scale_33, _input_zero_point_33, _input_dtype_33);  bert_encoder_layer_5_attention_output_layer_norm_bias = _input_scale_33 = _input_zero_point_33 = _input_dtype_33 = None\n",
      "<eval_with_key_9>:519: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_31 = torch.ops.quantized.add(mul_13, broadcast_to_24, _scale_42, _zero_point_42);  mul_13 = broadcast_to_24 = _scale_42 = _zero_point_42 = None\n",
      "<eval_with_key_9>:525: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_32 = torch.ops.quantized.add(bert_encoder_layer_5_output_dropout, add_31, _scale_43, _zero_point_43);  bert_encoder_layer_5_output_dropout = add_31 = _scale_43 = _zero_point_43 = None\n",
      "<eval_with_key_9>:530: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_41 = torch.quantize_per_tensor(bert_encoder_layer_5_output_layer_norm_weight, _input_scale_34, _input_zero_point_34, _input_dtype_34);  bert_encoder_layer_5_output_layer_norm_weight = _input_scale_34 = _input_zero_point_34 = _input_dtype_34 = None\n",
      "<eval_with_key_9>:533: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_14 = torch.ops.quantized.mul(add_32, quantize_per_tensor_41, _scale_44, _zero_point_44);  add_32 = quantize_per_tensor_41 = _scale_44 = _zero_point_44 = None\n",
      "<eval_with_key_9>:538: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_42 = torch.quantize_per_tensor(bert_encoder_layer_5_output_layer_norm_bias, _input_scale_35, _input_zero_point_35, _input_dtype_35);  bert_encoder_layer_5_output_layer_norm_bias = _input_scale_35 = _input_zero_point_35 = _input_dtype_35 = None\n",
      "<eval_with_key_9>:543: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_33 = torch.ops.quantized.add(mul_14, broadcast_to_25, _scale_45, _zero_point_45);  mul_14 = broadcast_to_25 = _scale_45 = _zero_point_45 = None\n",
      "<eval_with_key_9>:561: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_43 = torch.quantize_per_tensor(truediv_6, _input_scale_36, _input_zero_point_36, _input_dtype_36);  truediv_6 = _input_scale_36 = _input_zero_point_36 = _input_dtype_36 = None\n",
      "<eval_with_key_9>:566: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_34 = torch.ops.quantized.add(quantize_per_tensor_43, broadcast_to_7, _scale_46, _zero_point_46);  quantize_per_tensor_43 = broadcast_to_7 = _scale_46 = _zero_point_46 = None\n",
      "<eval_with_key_9>:578: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_44 = torch.quantize_per_tensor(view_27, bert_encoder_layer_6_attention_output_dense_input_scale_0, bert_encoder_layer_6_attention_output_dense_input_zero_point_0, bert_encoder_layer_6_attention_output_dense_input_dtype_0);  view_27 = bert_encoder_layer_6_attention_output_dense_input_scale_0 = bert_encoder_layer_6_attention_output_dense_input_zero_point_0 = bert_encoder_layer_6_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:583: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_35 = torch.ops.quantized.add(bert_encoder_layer_6_attention_output_dropout, add_33, _scale_47, _zero_point_47);  bert_encoder_layer_6_attention_output_dropout = add_33 = _scale_47 = _zero_point_47 = None\n",
      "<eval_with_key_9>:588: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_45 = torch.quantize_per_tensor(bert_encoder_layer_6_attention_output_layer_norm_weight, _input_scale_37, _input_zero_point_37, _input_dtype_37);  bert_encoder_layer_6_attention_output_layer_norm_weight = _input_scale_37 = _input_zero_point_37 = _input_dtype_37 = None\n",
      "<eval_with_key_9>:591: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_15 = torch.ops.quantized.mul(add_35, quantize_per_tensor_45, _scale_48, _zero_point_48);  add_35 = quantize_per_tensor_45 = _scale_48 = _zero_point_48 = None\n",
      "<eval_with_key_9>:596: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_46 = torch.quantize_per_tensor(bert_encoder_layer_6_attention_output_layer_norm_bias, _input_scale_38, _input_zero_point_38, _input_dtype_38);  bert_encoder_layer_6_attention_output_layer_norm_bias = _input_scale_38 = _input_zero_point_38 = _input_dtype_38 = None\n",
      "<eval_with_key_9>:601: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_36 = torch.ops.quantized.add(mul_15, broadcast_to_26, _scale_49, _zero_point_49);  mul_15 = broadcast_to_26 = _scale_49 = _zero_point_49 = None\n",
      "<eval_with_key_9>:607: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_37 = torch.ops.quantized.add(bert_encoder_layer_6_output_dropout, add_36, _scale_50, _zero_point_50);  bert_encoder_layer_6_output_dropout = add_36 = _scale_50 = _zero_point_50 = None\n",
      "<eval_with_key_9>:612: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_47 = torch.quantize_per_tensor(bert_encoder_layer_6_output_layer_norm_weight, _input_scale_39, _input_zero_point_39, _input_dtype_39);  bert_encoder_layer_6_output_layer_norm_weight = _input_scale_39 = _input_zero_point_39 = _input_dtype_39 = None\n",
      "<eval_with_key_9>:615: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_16 = torch.ops.quantized.mul(add_37, quantize_per_tensor_47, _scale_51, _zero_point_51);  add_37 = quantize_per_tensor_47 = _scale_51 = _zero_point_51 = None\n",
      "<eval_with_key_9>:620: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_48 = torch.quantize_per_tensor(bert_encoder_layer_6_output_layer_norm_bias, _input_scale_40, _input_zero_point_40, _input_dtype_40);  bert_encoder_layer_6_output_layer_norm_bias = _input_scale_40 = _input_zero_point_40 = _input_dtype_40 = None\n",
      "<eval_with_key_9>:625: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_38 = torch.ops.quantized.add(mul_16, broadcast_to_27, _scale_52, _zero_point_52);  mul_16 = broadcast_to_27 = _scale_52 = _zero_point_52 = None\n",
      "<eval_with_key_9>:643: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_49 = torch.quantize_per_tensor(truediv_7, _input_scale_41, _input_zero_point_41, _input_dtype_41);  truediv_7 = _input_scale_41 = _input_zero_point_41 = _input_dtype_41 = None\n",
      "<eval_with_key_9>:648: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_39 = torch.ops.quantized.add(quantize_per_tensor_49, broadcast_to_8, _scale_53, _zero_point_53);  quantize_per_tensor_49 = broadcast_to_8 = _scale_53 = _zero_point_53 = None\n",
      "<eval_with_key_9>:660: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_50 = torch.quantize_per_tensor(view_31, bert_encoder_layer_7_attention_output_dense_input_scale_0, bert_encoder_layer_7_attention_output_dense_input_zero_point_0, bert_encoder_layer_7_attention_output_dense_input_dtype_0);  view_31 = bert_encoder_layer_7_attention_output_dense_input_scale_0 = bert_encoder_layer_7_attention_output_dense_input_zero_point_0 = bert_encoder_layer_7_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:665: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_40 = torch.ops.quantized.add(bert_encoder_layer_7_attention_output_dropout, add_38, _scale_54, _zero_point_54);  bert_encoder_layer_7_attention_output_dropout = add_38 = _scale_54 = _zero_point_54 = None\n",
      "<eval_with_key_9>:670: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_51 = torch.quantize_per_tensor(bert_encoder_layer_7_attention_output_layer_norm_weight, _input_scale_42, _input_zero_point_42, _input_dtype_42);  bert_encoder_layer_7_attention_output_layer_norm_weight = _input_scale_42 = _input_zero_point_42 = _input_dtype_42 = None\n",
      "<eval_with_key_9>:673: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_17 = torch.ops.quantized.mul(add_40, quantize_per_tensor_51, _scale_55, _zero_point_55);  add_40 = quantize_per_tensor_51 = _scale_55 = _zero_point_55 = None\n",
      "<eval_with_key_9>:678: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_52 = torch.quantize_per_tensor(bert_encoder_layer_7_attention_output_layer_norm_bias, _input_scale_43, _input_zero_point_43, _input_dtype_43);  bert_encoder_layer_7_attention_output_layer_norm_bias = _input_scale_43 = _input_zero_point_43 = _input_dtype_43 = None\n",
      "<eval_with_key_9>:683: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_41 = torch.ops.quantized.add(mul_17, broadcast_to_28, _scale_56, _zero_point_56);  mul_17 = broadcast_to_28 = _scale_56 = _zero_point_56 = None\n",
      "<eval_with_key_9>:689: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_42 = torch.ops.quantized.add(bert_encoder_layer_7_output_dropout, add_41, _scale_57, _zero_point_57);  bert_encoder_layer_7_output_dropout = add_41 = _scale_57 = _zero_point_57 = None\n",
      "<eval_with_key_9>:694: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_53 = torch.quantize_per_tensor(bert_encoder_layer_7_output_layer_norm_weight, _input_scale_44, _input_zero_point_44, _input_dtype_44);  bert_encoder_layer_7_output_layer_norm_weight = _input_scale_44 = _input_zero_point_44 = _input_dtype_44 = None\n",
      "<eval_with_key_9>:697: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_18 = torch.ops.quantized.mul(add_42, quantize_per_tensor_53, _scale_58, _zero_point_58);  add_42 = quantize_per_tensor_53 = _scale_58 = _zero_point_58 = None\n",
      "<eval_with_key_9>:702: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_54 = torch.quantize_per_tensor(bert_encoder_layer_7_output_layer_norm_bias, _input_scale_45, _input_zero_point_45, _input_dtype_45);  bert_encoder_layer_7_output_layer_norm_bias = _input_scale_45 = _input_zero_point_45 = _input_dtype_45 = None\n",
      "<eval_with_key_9>:707: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_43 = torch.ops.quantized.add(mul_18, broadcast_to_29, _scale_59, _zero_point_59);  mul_18 = broadcast_to_29 = _scale_59 = _zero_point_59 = None\n",
      "<eval_with_key_9>:725: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_55 = torch.quantize_per_tensor(truediv_8, _input_scale_46, _input_zero_point_46, _input_dtype_46);  truediv_8 = _input_scale_46 = _input_zero_point_46 = _input_dtype_46 = None\n",
      "<eval_with_key_9>:730: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_44 = torch.ops.quantized.add(quantize_per_tensor_55, broadcast_to_9, _scale_60, _zero_point_60);  quantize_per_tensor_55 = broadcast_to_9 = _scale_60 = _zero_point_60 = None\n",
      "<eval_with_key_9>:742: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_56 = torch.quantize_per_tensor(view_35, bert_encoder_layer_8_attention_output_dense_input_scale_0, bert_encoder_layer_8_attention_output_dense_input_zero_point_0, bert_encoder_layer_8_attention_output_dense_input_dtype_0);  view_35 = bert_encoder_layer_8_attention_output_dense_input_scale_0 = bert_encoder_layer_8_attention_output_dense_input_zero_point_0 = bert_encoder_layer_8_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:747: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_45 = torch.ops.quantized.add(bert_encoder_layer_8_attention_output_dropout, add_43, _scale_61, _zero_point_61);  bert_encoder_layer_8_attention_output_dropout = add_43 = _scale_61 = _zero_point_61 = None\n",
      "<eval_with_key_9>:752: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_57 = torch.quantize_per_tensor(bert_encoder_layer_8_attention_output_layer_norm_weight, _input_scale_47, _input_zero_point_47, _input_dtype_47);  bert_encoder_layer_8_attention_output_layer_norm_weight = _input_scale_47 = _input_zero_point_47 = _input_dtype_47 = None\n",
      "<eval_with_key_9>:755: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_19 = torch.ops.quantized.mul(add_45, quantize_per_tensor_57, _scale_62, _zero_point_62);  add_45 = quantize_per_tensor_57 = _scale_62 = _zero_point_62 = None\n",
      "<eval_with_key_9>:760: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_58 = torch.quantize_per_tensor(bert_encoder_layer_8_attention_output_layer_norm_bias, _input_scale_48, _input_zero_point_48, _input_dtype_48);  bert_encoder_layer_8_attention_output_layer_norm_bias = _input_scale_48 = _input_zero_point_48 = _input_dtype_48 = None\n",
      "<eval_with_key_9>:765: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_46 = torch.ops.quantized.add(mul_19, broadcast_to_30, _scale_63, _zero_point_63);  mul_19 = broadcast_to_30 = _scale_63 = _zero_point_63 = None\n",
      "<eval_with_key_9>:771: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_47 = torch.ops.quantized.add(bert_encoder_layer_8_output_dropout, add_46, _scale_64, _zero_point_64);  bert_encoder_layer_8_output_dropout = add_46 = _scale_64 = _zero_point_64 = None\n",
      "<eval_with_key_9>:776: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_59 = torch.quantize_per_tensor(bert_encoder_layer_8_output_layer_norm_weight, _input_scale_49, _input_zero_point_49, _input_dtype_49);  bert_encoder_layer_8_output_layer_norm_weight = _input_scale_49 = _input_zero_point_49 = _input_dtype_49 = None\n",
      "<eval_with_key_9>:779: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_20 = torch.ops.quantized.mul(add_47, quantize_per_tensor_59, _scale_65, _zero_point_65);  add_47 = quantize_per_tensor_59 = _scale_65 = _zero_point_65 = None\n",
      "<eval_with_key_9>:784: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_60 = torch.quantize_per_tensor(bert_encoder_layer_8_output_layer_norm_bias, _input_scale_50, _input_zero_point_50, _input_dtype_50);  bert_encoder_layer_8_output_layer_norm_bias = _input_scale_50 = _input_zero_point_50 = _input_dtype_50 = None\n",
      "<eval_with_key_9>:789: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_48 = torch.ops.quantized.add(mul_20, broadcast_to_31, _scale_66, _zero_point_66);  mul_20 = broadcast_to_31 = _scale_66 = _zero_point_66 = None\n",
      "<eval_with_key_9>:807: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_61 = torch.quantize_per_tensor(truediv_9, _input_scale_51, _input_zero_point_51, _input_dtype_51);  truediv_9 = _input_scale_51 = _input_zero_point_51 = _input_dtype_51 = None\n",
      "<eval_with_key_9>:812: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_49 = torch.ops.quantized.add(quantize_per_tensor_61, broadcast_to_10, _scale_67, _zero_point_67);  quantize_per_tensor_61 = broadcast_to_10 = _scale_67 = _zero_point_67 = None\n",
      "<eval_with_key_9>:824: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_62 = torch.quantize_per_tensor(view_39, bert_encoder_layer_9_attention_output_dense_input_scale_0, bert_encoder_layer_9_attention_output_dense_input_zero_point_0, bert_encoder_layer_9_attention_output_dense_input_dtype_0);  view_39 = bert_encoder_layer_9_attention_output_dense_input_scale_0 = bert_encoder_layer_9_attention_output_dense_input_zero_point_0 = bert_encoder_layer_9_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:829: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_50 = torch.ops.quantized.add(bert_encoder_layer_9_attention_output_dropout, add_48, _scale_68, _zero_point_68);  bert_encoder_layer_9_attention_output_dropout = add_48 = _scale_68 = _zero_point_68 = None\n",
      "<eval_with_key_9>:834: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_63 = torch.quantize_per_tensor(bert_encoder_layer_9_attention_output_layer_norm_weight, _input_scale_52, _input_zero_point_52, _input_dtype_52);  bert_encoder_layer_9_attention_output_layer_norm_weight = _input_scale_52 = _input_zero_point_52 = _input_dtype_52 = None\n",
      "<eval_with_key_9>:837: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_21 = torch.ops.quantized.mul(add_50, quantize_per_tensor_63, _scale_69, _zero_point_69);  add_50 = quantize_per_tensor_63 = _scale_69 = _zero_point_69 = None\n",
      "<eval_with_key_9>:842: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_64 = torch.quantize_per_tensor(bert_encoder_layer_9_attention_output_layer_norm_bias, _input_scale_53, _input_zero_point_53, _input_dtype_53);  bert_encoder_layer_9_attention_output_layer_norm_bias = _input_scale_53 = _input_zero_point_53 = _input_dtype_53 = None\n",
      "<eval_with_key_9>:847: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_51 = torch.ops.quantized.add(mul_21, broadcast_to_32, _scale_70, _zero_point_70);  mul_21 = broadcast_to_32 = _scale_70 = _zero_point_70 = None\n",
      "<eval_with_key_9>:853: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_52 = torch.ops.quantized.add(bert_encoder_layer_9_output_dropout, add_51, _scale_71, _zero_point_71);  bert_encoder_layer_9_output_dropout = add_51 = _scale_71 = _zero_point_71 = None\n",
      "<eval_with_key_9>:858: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_65 = torch.quantize_per_tensor(bert_encoder_layer_9_output_layer_norm_weight, _input_scale_54, _input_zero_point_54, _input_dtype_54);  bert_encoder_layer_9_output_layer_norm_weight = _input_scale_54 = _input_zero_point_54 = _input_dtype_54 = None\n",
      "<eval_with_key_9>:861: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_22 = torch.ops.quantized.mul(add_52, quantize_per_tensor_65, _scale_72, _zero_point_72);  add_52 = quantize_per_tensor_65 = _scale_72 = _zero_point_72 = None\n",
      "<eval_with_key_9>:866: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_66 = torch.quantize_per_tensor(bert_encoder_layer_9_output_layer_norm_bias, _input_scale_55, _input_zero_point_55, _input_dtype_55);  bert_encoder_layer_9_output_layer_norm_bias = _input_scale_55 = _input_zero_point_55 = _input_dtype_55 = None\n",
      "<eval_with_key_9>:871: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_53 = torch.ops.quantized.add(mul_22, broadcast_to_33, _scale_73, _zero_point_73);  mul_22 = broadcast_to_33 = _scale_73 = _zero_point_73 = None\n",
      "<eval_with_key_9>:889: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_67 = torch.quantize_per_tensor(truediv_10, _input_scale_56, _input_zero_point_56, _input_dtype_56);  truediv_10 = _input_scale_56 = _input_zero_point_56 = _input_dtype_56 = None\n",
      "<eval_with_key_9>:894: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_54 = torch.ops.quantized.add(quantize_per_tensor_67, broadcast_to_11, _scale_74, _zero_point_74);  quantize_per_tensor_67 = broadcast_to_11 = _scale_74 = _zero_point_74 = None\n",
      "<eval_with_key_9>:906: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_68 = torch.quantize_per_tensor(view_43, bert_encoder_layer_10_attention_output_dense_input_scale_0, bert_encoder_layer_10_attention_output_dense_input_zero_point_0, bert_encoder_layer_10_attention_output_dense_input_dtype_0);  view_43 = bert_encoder_layer_10_attention_output_dense_input_scale_0 = bert_encoder_layer_10_attention_output_dense_input_zero_point_0 = bert_encoder_layer_10_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:911: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_55 = torch.ops.quantized.add(bert_encoder_layer_10_attention_output_dropout, add_53, _scale_75, _zero_point_75);  bert_encoder_layer_10_attention_output_dropout = add_53 = _scale_75 = _zero_point_75 = None\n",
      "<eval_with_key_9>:916: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_69 = torch.quantize_per_tensor(bert_encoder_layer_10_attention_output_layer_norm_weight, _input_scale_57, _input_zero_point_57, _input_dtype_57);  bert_encoder_layer_10_attention_output_layer_norm_weight = _input_scale_57 = _input_zero_point_57 = _input_dtype_57 = None\n",
      "<eval_with_key_9>:919: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_23 = torch.ops.quantized.mul(add_55, quantize_per_tensor_69, _scale_76, _zero_point_76);  add_55 = quantize_per_tensor_69 = _scale_76 = _zero_point_76 = None\n",
      "<eval_with_key_9>:924: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_70 = torch.quantize_per_tensor(bert_encoder_layer_10_attention_output_layer_norm_bias, _input_scale_58, _input_zero_point_58, _input_dtype_58);  bert_encoder_layer_10_attention_output_layer_norm_bias = _input_scale_58 = _input_zero_point_58 = _input_dtype_58 = None\n",
      "<eval_with_key_9>:929: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_56 = torch.ops.quantized.add(mul_23, broadcast_to_34, _scale_77, _zero_point_77);  mul_23 = broadcast_to_34 = _scale_77 = _zero_point_77 = None\n",
      "<eval_with_key_9>:935: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_57 = torch.ops.quantized.add(bert_encoder_layer_10_output_dropout, add_56, _scale_78, _zero_point_78);  bert_encoder_layer_10_output_dropout = add_56 = _scale_78 = _zero_point_78 = None\n",
      "<eval_with_key_9>:940: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_71 = torch.quantize_per_tensor(bert_encoder_layer_10_output_layer_norm_weight, _input_scale_59, _input_zero_point_59, _input_dtype_59);  bert_encoder_layer_10_output_layer_norm_weight = _input_scale_59 = _input_zero_point_59 = _input_dtype_59 = None\n",
      "<eval_with_key_9>:943: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_24 = torch.ops.quantized.mul(add_57, quantize_per_tensor_71, _scale_79, _zero_point_79);  add_57 = quantize_per_tensor_71 = _scale_79 = _zero_point_79 = None\n",
      "<eval_with_key_9>:948: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_72 = torch.quantize_per_tensor(bert_encoder_layer_10_output_layer_norm_bias, _input_scale_60, _input_zero_point_60, _input_dtype_60);  bert_encoder_layer_10_output_layer_norm_bias = _input_scale_60 = _input_zero_point_60 = _input_dtype_60 = None\n",
      "<eval_with_key_9>:953: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_58 = torch.ops.quantized.add(mul_24, broadcast_to_35, _scale_80, _zero_point_80);  mul_24 = broadcast_to_35 = _scale_80 = _zero_point_80 = None\n",
      "<eval_with_key_9>:971: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_73 = torch.quantize_per_tensor(truediv_11, _input_scale_61, _input_zero_point_61, _input_dtype_61);  truediv_11 = _input_scale_61 = _input_zero_point_61 = _input_dtype_61 = None\n",
      "<eval_with_key_9>:976: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_59 = torch.ops.quantized.add(quantize_per_tensor_73, broadcast_to_12, _scale_81, _zero_point_81);  quantize_per_tensor_73 = broadcast_to_12 = _scale_81 = _zero_point_81 = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<eval_with_key_9>:988: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_74 = torch.quantize_per_tensor(view_47, bert_encoder_layer_11_attention_output_dense_input_scale_0, bert_encoder_layer_11_attention_output_dense_input_zero_point_0, bert_encoder_layer_11_attention_output_dense_input_dtype_0);  view_47 = bert_encoder_layer_11_attention_output_dense_input_scale_0 = bert_encoder_layer_11_attention_output_dense_input_zero_point_0 = bert_encoder_layer_11_attention_output_dense_input_dtype_0 = None\n",
      "<eval_with_key_9>:993: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_60 = torch.ops.quantized.add(bert_encoder_layer_11_attention_output_dropout, add_58, _scale_82, _zero_point_82);  bert_encoder_layer_11_attention_output_dropout = add_58 = _scale_82 = _zero_point_82 = None\n",
      "<eval_with_key_9>:998: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_75 = torch.quantize_per_tensor(bert_encoder_layer_11_attention_output_layer_norm_weight, _input_scale_62, _input_zero_point_62, _input_dtype_62);  bert_encoder_layer_11_attention_output_layer_norm_weight = _input_scale_62 = _input_zero_point_62 = _input_dtype_62 = None\n",
      "<eval_with_key_9>:1001: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_25 = torch.ops.quantized.mul(add_60, quantize_per_tensor_75, _scale_83, _zero_point_83);  add_60 = quantize_per_tensor_75 = _scale_83 = _zero_point_83 = None\n",
      "<eval_with_key_9>:1006: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_76 = torch.quantize_per_tensor(bert_encoder_layer_11_attention_output_layer_norm_bias, _input_scale_63, _input_zero_point_63, _input_dtype_63);  bert_encoder_layer_11_attention_output_layer_norm_bias = _input_scale_63 = _input_zero_point_63 = _input_dtype_63 = None\n",
      "<eval_with_key_9>:1011: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_61 = torch.ops.quantized.add(mul_25, broadcast_to_36, _scale_84, _zero_point_84);  mul_25 = broadcast_to_36 = _scale_84 = _zero_point_84 = None\n",
      "<eval_with_key_9>:1017: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_62 = torch.ops.quantized.add(bert_encoder_layer_11_output_dropout, add_61, _scale_85, _zero_point_85);  bert_encoder_layer_11_output_dropout = add_61 = _scale_85 = _zero_point_85 = None\n",
      "<eval_with_key_9>:1022: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_77 = torch.quantize_per_tensor(bert_encoder_layer_11_output_layer_norm_weight, _input_scale_64, _input_zero_point_64, _input_dtype_64);  bert_encoder_layer_11_output_layer_norm_weight = _input_scale_64 = _input_zero_point_64 = _input_dtype_64 = None\n",
      "<eval_with_key_9>:1025: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mul_26 = torch.ops.quantized.mul(add_62, quantize_per_tensor_77, _scale_86, _zero_point_86);  add_62 = quantize_per_tensor_77 = _scale_86 = _zero_point_86 = None\n",
      "<eval_with_key_9>:1030: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quantize_per_tensor_78 = torch.quantize_per_tensor(bert_encoder_layer_11_output_layer_norm_bias, _input_scale_65, _input_zero_point_65, _input_dtype_65);  bert_encoder_layer_11_output_layer_norm_bias = _input_scale_65 = _input_zero_point_65 = _input_dtype_65 = None\n",
      "<eval_with_key_9>:1035: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  add_63 = torch.ops.quantized.add(mul_26, broadcast_to_37, _scale_87, _zero_point_87);  mul_26 = broadcast_to_37 = _scale_87 = _zero_point_87 = None\n"
     ]
    }
   ],
   "source": [
    "traced_model_int8 = torch.jit.trace(model_int8, tuple(model_int8.dummy_inputs.values()), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84d04c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model_int8_output = traced_model_int8(*tuple(model_int8.dummy_inputs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29d593f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = len(model_int8_output)\n",
    "for i in range(num_outputs):\n",
    "    if not torch.allclose(model_int8_output[i], traced_model_int8_output[i]):\n",
    "        print(f\"The {i}th outputs do not match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "575cff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(traced_model_int8, \"quantized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d780e64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=GraphModule\n",
       "  (bert): RecursiveScriptModule(\n",
       "    original_name=Module\n",
       "    (embeddings): RecursiveScriptModule(\n",
       "      original_name=Module\n",
       "      (word_embeddings): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (_packed_params): RecursiveScriptModule(original_name=EmbeddingPackedParams)\n",
       "      )\n",
       "      (token_type_embeddings): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (_packed_params): RecursiveScriptModule(original_name=EmbeddingPackedParams)\n",
       "      )\n",
       "      (position_embeddings): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (_packed_params): RecursiveScriptModule(original_name=EmbeddingPackedParams)\n",
       "      )\n",
       "      (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "      (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    )\n",
       "    (encoder): RecursiveScriptModule(\n",
       "      original_name=Module\n",
       "      (layer): RecursiveScriptModule(\n",
       "        original_name=Module\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "        (2): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "        (3): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "        (4): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "        (5): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "        (6): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "        (7): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "        (8): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "        (9): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "        (10): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "        (11): RecursiveScriptModule(\n",
       "          original_name=Module\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (query): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (key): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (value): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (mod): RecursiveScriptModule(original_name=Softmax)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=Module\n",
       "              (dense): RecursiveScriptModule(\n",
       "                original_name=Linear\n",
       "                (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "              )\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=LinearReLU\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=Module\n",
       "            (dense): RecursiveScriptModule(\n",
       "              original_name=Linear\n",
       "              (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "            )\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=Module)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): RecursiveScriptModule(\n",
       "    original_name=Linear\n",
       "    (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model = torch.jit.load(\"quantized.pt\")\n",
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ec239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nn_pruning]",
   "language": "python",
   "name": "conda-env-nn_pruning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
