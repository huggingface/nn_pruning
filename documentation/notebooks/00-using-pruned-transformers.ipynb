{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SquadV1 inference with a pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at madlag/bert-base-uncased-squadv1-x2.44-f87.7-d26-hybrid-filled-v1 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-base parameters: 110.0M\n",
      "removed heads 0, total_heads=64, percentage removed=0.0\n",
      "bert.encoder.layer.0.intermediate.dense, sparsity = 82.03\n",
      "bert.encoder.layer.0.output.dense, sparsity = 82.03\n",
      "bert.encoder.layer.1.intermediate.dense, sparsity = 76.66\n",
      "bert.encoder.layer.1.output.dense, sparsity = 76.66\n",
      "bert.encoder.layer.2.intermediate.dense, sparsity = 74.12\n",
      "bert.encoder.layer.2.output.dense, sparsity = 74.12\n",
      "bert.encoder.layer.3.intermediate.dense, sparsity = 74.32\n",
      "bert.encoder.layer.3.output.dense, sparsity = 74.32\n",
      "bert.encoder.layer.4.intermediate.dense, sparsity = 72.88\n",
      "bert.encoder.layer.4.output.dense, sparsity = 72.88\n",
      "bert.encoder.layer.5.intermediate.dense, sparsity = 74.22\n",
      "bert.encoder.layer.5.output.dense, sparsity = 74.22\n",
      "bert.encoder.layer.6.intermediate.dense, sparsity = 79.82\n",
      "bert.encoder.layer.6.output.dense, sparsity = 79.82\n",
      "bert.encoder.layer.7.intermediate.dense, sparsity = 84.83\n",
      "bert.encoder.layer.7.output.dense, sparsity = 84.83\n",
      "bert.encoder.layer.8.intermediate.dense, sparsity = 90.79\n",
      "bert.encoder.layer.8.output.dense, sparsity = 90.79\n",
      "bert.encoder.layer.9.intermediate.dense, sparsity = 96.35\n",
      "bert.encoder.layer.9.output.dense, sparsity = 96.35\n",
      "bert.encoder.layer.10.intermediate.dense, sparsity = 96.88\n",
      "bert.encoder.layer.10.output.dense, sparsity = 96.88\n",
      "bert.encoder.layer.11.intermediate.dense, sparsity = 92.25\n",
      "bert.encoder.layer.11.output.dense, sparsity = 92.25\n",
      "Parameters count after optimization=46.2M\n",
      "Reduction of the total number of parameters compared to BERT-base:2.38X\n",
      "\n",
      "Predictions {'score': 0.5601632595062256, 'start': 96, 'end': 134, 'answer': 'a Polish composer and virtuoso pianist'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from nn_pruning.inference_model_patcher import optimize_model\n",
    "\n",
    "# BERT-base uncased finetuned on SQuAD, speedup is 2.44, F1=87.7, 26% of linear layer parameters remaining,\n",
    "# with hybrid-pruning + final fill -> dense matrices\n",
    "MODEL_NAME = \"madlag/bert-base-uncased-squadv1-x2.44-f87.7-d26-hybrid-filled-v1\"\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=MODEL_NAME,\n",
    ")\n",
    "\n",
    "# Original BERT-base size\n",
    "original_bert = 110E6\n",
    "print(f\"BERT-base parameters: {original_bert/1E6:0.1f}M\")\n",
    "\n",
    "# Optimize the model: this just removes the empty parts of the model (lines/columns), as we\n",
    "# cannot currently store the shrunk version on disk in a huggingface transformers compatible format\n",
    "qa_pipeline.model = optimize_model(qa_pipeline.model, \"dense\")\n",
    "\n",
    "# Check the new size\n",
    "new_count = int(qa_pipeline.model.num_parameters())\n",
    "print(f\"Parameters count after optimization={new_count / 1E6:0.1f}M\")\n",
    "print(f\"Reduction of the total number of parameters compared to BERT-base:{original_bert / new_count:0.2f}X\")\n",
    "\n",
    "# Use the model as usual, it's just 2.44X faster!\n",
    "predictions = qa_pipeline({\n",
    "    'context': \"Frédéric François Chopin, born Fryderyk Franciszek Chopin (1 March 1810 – 17 October 1849), was a Polish composer and virtuoso pianist of the Romantic era who wrote primarily for solo piano.\",\n",
    "    'question': \"Who is Frederic Chopin?\",\n",
    "})\n",
    "print()\n",
    "print(\"Predictions\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the size of the model linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed heads 0, total_heads=64, percentage removed=0.0\n",
      "bert.encoder.layer.0.intermediate.dense, sparsity = 82.03\n",
      "bert.encoder.layer.0.output.dense, sparsity = 82.03\n",
      "bert.encoder.layer.1.intermediate.dense, sparsity = 76.66\n",
      "bert.encoder.layer.1.output.dense, sparsity = 76.66\n",
      "bert.encoder.layer.2.intermediate.dense, sparsity = 74.12\n",
      "bert.encoder.layer.2.output.dense, sparsity = 74.12\n",
      "bert.encoder.layer.3.intermediate.dense, sparsity = 74.32\n",
      "bert.encoder.layer.3.output.dense, sparsity = 74.32\n",
      "bert.encoder.layer.4.intermediate.dense, sparsity = 72.88\n",
      "bert.encoder.layer.4.output.dense, sparsity = 72.88\n",
      "bert.encoder.layer.5.intermediate.dense, sparsity = 74.22\n",
      "bert.encoder.layer.5.output.dense, sparsity = 74.22\n",
      "bert.encoder.layer.6.intermediate.dense, sparsity = 79.82\n",
      "bert.encoder.layer.6.output.dense, sparsity = 79.82\n",
      "bert.encoder.layer.7.intermediate.dense, sparsity = 84.83\n",
      "bert.encoder.layer.7.output.dense, sparsity = 84.83\n",
      "bert.encoder.layer.8.intermediate.dense, sparsity = 90.79\n",
      "bert.encoder.layer.8.output.dense, sparsity = 90.79\n",
      "bert.encoder.layer.9.intermediate.dense, sparsity = 96.35\n",
      "bert.encoder.layer.9.output.dense, sparsity = 96.35\n",
      "bert.encoder.layer.10.intermediate.dense, sparsity = 96.88\n",
      "bert.encoder.layer.10.output.dense, sparsity = 96.88\n",
      "bert.encoder.layer.11.intermediate.dense, sparsity = 92.25\n",
      "bert.encoder.layer.11.output.dense, sparsity = 92.25\n",
      "\n",
      "Reduction of linear layers:3.11X\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "from nn_pruning.inference_model_patcher import optimize_model\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"madlag/bert-base-uncased-squadv1-x2.44-f87.7-d26-hybrid-filled-v1\")\n",
    "\n",
    "\n",
    "def compute_size(model):\n",
    "    elems = 0\n",
    "    for k, v in model.named_parameters():\n",
    "        if \"LayerNorm\" not in k and \"encoder\" in k and \"weight\" in k:\n",
    "            elems += v.numel()\n",
    "    return elems\n",
    "\n",
    "original_count = compute_size(model)\n",
    "\n",
    "# Optimize the model\n",
    "model = optimize_model(model, mode=\"dense\", clone=False)\n",
    "\n",
    "new_count = compute_size(model)\n",
    "\n",
    "print()\n",
    "print(f\"Reduction of linear layers:{original_count / new_count:0.2f}X\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-base Size, Model Size, Layer Name\n",
      "\n",
      "Layer 0\n",
      "[768, 768] => [256, 768], attention.self.query.weight\n",
      "[768] => [256], attention.self.query.bias\n",
      "[768, 768] => [256, 768], attention.self.key.weight\n",
      "[768] => [256], attention.self.key.bias\n",
      "[768, 768] => [256, 768], attention.self.value.weight\n",
      "[768] => [256], attention.self.value.bias\n",
      "[768, 768] => [768, 256], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [552, 768], intermediate.dense.weight\n",
      "[768] => [552], intermediate.dense.bias\n",
      "[768, 3072] => [768, 552], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n",
      "\n",
      "Layer 1\n",
      "[768, 768] => [256, 768], attention.self.query.weight\n",
      "[768] => [256], attention.self.query.bias\n",
      "[768, 768] => [256, 768], attention.self.key.weight\n",
      "[768] => [256], attention.self.key.bias\n",
      "[768, 768] => [256, 768], attention.self.value.weight\n",
      "[768] => [256], attention.self.value.bias\n",
      "[768, 768] => [768, 256], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [717, 768], intermediate.dense.weight\n",
      "[768] => [717], intermediate.dense.bias\n",
      "[768, 3072] => [768, 717], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n",
      "\n",
      "Layer 2\n",
      "[768, 768] => [384, 768], attention.self.query.weight\n",
      "[768] => [384], attention.self.query.bias\n",
      "[768, 768] => [384, 768], attention.self.key.weight\n",
      "[768] => [384], attention.self.key.bias\n",
      "[768, 768] => [384, 768], attention.self.value.weight\n",
      "[768] => [384], attention.self.value.bias\n",
      "[768, 768] => [768, 384], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [795, 768], intermediate.dense.weight\n",
      "[768] => [795], intermediate.dense.bias\n",
      "[768, 3072] => [768, 795], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n",
      "\n",
      "Layer 3\n",
      "[768, 768] => [448, 768], attention.self.query.weight\n",
      "[768] => [448], attention.self.query.bias\n",
      "[768, 768] => [448, 768], attention.self.key.weight\n",
      "[768] => [448], attention.self.key.bias\n",
      "[768, 768] => [448, 768], attention.self.value.weight\n",
      "[768] => [448], attention.self.value.bias\n",
      "[768, 768] => [768, 448], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [789, 768], intermediate.dense.weight\n",
      "[768] => [789], intermediate.dense.bias\n",
      "[768, 3072] => [768, 789], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n",
      "\n",
      "Layer 4\n",
      "[768, 768] => [448, 768], attention.self.query.weight\n",
      "[768] => [448], attention.self.query.bias\n",
      "[768, 768] => [448, 768], attention.self.key.weight\n",
      "[768] => [448], attention.self.key.bias\n",
      "[768, 768] => [448, 768], attention.self.value.weight\n",
      "[768] => [448], attention.self.value.bias\n",
      "[768, 768] => [768, 448], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [833, 768], intermediate.dense.weight\n",
      "[768] => [833], intermediate.dense.bias\n",
      "[768, 3072] => [768, 833], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n",
      "\n",
      "Layer 5\n",
      "[768, 768] => [320, 768], attention.self.query.weight\n",
      "[768] => [320], attention.self.query.bias\n",
      "[768, 768] => [320, 768], attention.self.key.weight\n",
      "[768] => [320], attention.self.key.bias\n",
      "[768, 768] => [320, 768], attention.self.value.weight\n",
      "[768] => [320], attention.self.value.bias\n",
      "[768, 768] => [768, 320], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [792, 768], intermediate.dense.weight\n",
      "[768] => [792], intermediate.dense.bias\n",
      "[768, 3072] => [768, 792], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n",
      "\n",
      "Layer 6\n",
      "[768, 768] => [384, 768], attention.self.query.weight\n",
      "[768] => [384], attention.self.query.bias\n",
      "[768, 768] => [384, 768], attention.self.key.weight\n",
      "[768] => [384], attention.self.key.bias\n",
      "[768, 768] => [384, 768], attention.self.value.weight\n",
      "[768] => [384], attention.self.value.bias\n",
      "[768, 768] => [768, 384], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [620, 768], intermediate.dense.weight\n",
      "[768] => [620], intermediate.dense.bias\n",
      "[768, 3072] => [768, 620], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n",
      "\n",
      "Layer 7\n",
      "[768, 768] => [448, 768], attention.self.query.weight\n",
      "[768] => [448], attention.self.query.bias\n",
      "[768, 768] => [448, 768], attention.self.key.weight\n",
      "[768] => [448], attention.self.key.bias\n",
      "[768, 768] => [448, 768], attention.self.value.weight\n",
      "[768] => [448], attention.self.value.bias\n",
      "[768, 768] => [768, 448], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [466, 768], intermediate.dense.weight\n",
      "[768] => [466], intermediate.dense.bias\n",
      "[768, 3072] => [768, 466], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n",
      "\n",
      "Layer 8\n",
      "[768, 768] => [320, 768], attention.self.query.weight\n",
      "[768] => [320], attention.self.query.bias\n",
      "[768, 768] => [320, 768], attention.self.key.weight\n",
      "[768] => [320], attention.self.key.bias\n",
      "[768, 768] => [320, 768], attention.self.value.weight\n",
      "[768] => [320], attention.self.value.bias\n",
      "[768, 768] => [768, 320], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [283, 768], intermediate.dense.weight\n",
      "[768] => [283], intermediate.dense.bias\n",
      "[768, 3072] => [768, 283], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n",
      "\n",
      "Layer 9\n",
      "[768, 768] => [320, 768], attention.self.query.weight\n",
      "[768] => [320], attention.self.query.bias\n",
      "[768, 768] => [320, 768], attention.self.key.weight\n",
      "[768] => [320], attention.self.key.bias\n",
      "[768, 768] => [320, 768], attention.self.value.weight\n",
      "[768] => [320], attention.self.value.bias\n",
      "[768, 768] => [768, 320], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [112, 768], intermediate.dense.weight\n",
      "[768] => [112], intermediate.dense.bias\n",
      "[768, 3072] => [768, 112], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n",
      "\n",
      "Layer 10\n",
      "[768, 768] => [320, 768], attention.self.query.weight\n",
      "[768] => [320], attention.self.query.bias\n",
      "[768, 768] => [320, 768], attention.self.key.weight\n",
      "[768] => [320], attention.self.key.bias\n",
      "[768, 768] => [320, 768], attention.self.value.weight\n",
      "[768] => [320], attention.self.value.bias\n",
      "[768, 768] => [768, 320], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [96, 768], intermediate.dense.weight\n",
      "[768] => [96], intermediate.dense.bias\n",
      "[768, 3072] => [768, 96], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n",
      "\n",
      "Layer 11\n",
      "[768, 768] => [192, 768], attention.self.query.weight\n",
      "[768] => [192], attention.self.query.bias\n",
      "[768, 768] => [192, 768], attention.self.key.weight\n",
      "[768] => [192], attention.self.key.bias\n",
      "[768, 768] => [192, 768], attention.self.value.weight\n",
      "[768] => [192], attention.self.value.bias\n",
      "[768, 768] => [768, 192], attention.output.dense.weight\n",
      "[768] => [768], attention.output.dense.bias\n",
      "[3072, 768] => [238, 768], intermediate.dense.weight\n",
      "[768] => [238], intermediate.dense.bias\n",
      "[768, 3072] => [768, 238], output.dense.weight\n",
      "[768] => [768], output.dense.bias\n"
     ]
    }
   ],
   "source": [
    "def print_sizes(model):\n",
    "    current_layer_index = None\n",
    "    for k, v in model.named_parameters():\n",
    "        if \"encoder\" not in k or \"LayerNorm\" in k:\n",
    "            continue\n",
    "        layer_index = k.split(\".\")[3]\n",
    "        if layer_index != current_layer_index:\n",
    "            print(f\"\\nLayer {layer_index}\")\n",
    "            current_layer_index = layer_index\n",
    "            \n",
    "        k = \".\".join(k.split(\".\")[4:])\n",
    "                \n",
    "        if \"weight\" in k:\n",
    "            if \"attention\" in k:\n",
    "                th_size = [768, 768]\n",
    "            else:\n",
    "                if \"intermediate\" in k:\n",
    "                    th_size = [768*4, 768]\n",
    "                else:\n",
    "                    th_size = [768, 768 * 4]\n",
    "        elif \"bias\" in k:\n",
    "            th_size = [768]\n",
    "        else:\n",
    "            raise ValueError(\"unsupported case\")\n",
    "        print(f\"{th_size} => {list(v.shape)}, {k}\")\n",
    "\n",
    "print(\"BERT-base Size, Model Size, Layer Name\")\n",
    "print_sizes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
