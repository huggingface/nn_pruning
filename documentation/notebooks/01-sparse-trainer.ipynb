{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Pruning with a Sparse Trainer\n",
    "\n",
    "> How to make sparse and fast models with a mix of structured and unstructured pruning\n",
    "\n",
    "In this tutorial, we'll see how `nn_pruning` combines techniques from [movement pruning](https://arxiv.org/abs/2005.07683) and structured pruning to produce compact Transformers that can run inference faster than their dense counterparts, with little impact on accuracy. This tutorial is aimed at those who are familiar with the `transformers.Trainer` - if you're not, you can check out the [documentation](https://huggingface.co/transformers/main_classes/trainer.html?highlight=trainer#trainer) and `transformers` [examples](https://huggingface.co/transformers/examples.html#the-big-table-of-tasks) to see how it works. Let's get started! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transformers v4.3.3 and datasets v1.4.1 and torch v1.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "datasets.logging.set_verbosity_error()\n",
    "transformers.logging.set_verbosity_error()\n",
    "print(f\"Using transformers v{transformers.__version__} and datasets v{datasets.__version__} and torch v{torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show `nn_pruning` in action, we'll use the [BoolQ dataset](https://arxiv.org/abs/1905.10044) which consists of naturally occurring yes/no questions concerning a passage of text. We can use the `datasets` library to load the dataset from the [Hugging Face Hub](https://huggingface.co/) as part of the [SuperGLUE benchmark](https://huggingface.co/datasets/super_glue):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 9427\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 3270\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 3245\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "boolq = load_dataset(\"super_glue\", \"boolq\")\n",
    "boolq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'label': 1,\n",
       " 'passage': 'Persian language -- Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.',\n",
       " 'question': 'do iran and afghanistan speak the same language'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolq['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that we're given a `question` about a `passage` of text, and the answer is given a value of 0 (false) / 1 (true) in the `label` field. To help the trainer automatically detect the labels, let's rename the column as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq.rename_column_(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the question-answer pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can fine-prune any models, the first thing we need to do is tokenize and encode the `question` and `passage` fields of each example. Currently, `nn_pruning` supports fine-pruning for BERT models so we'll use BERT-base and load up the tokenizer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenize our inputs, we'll pass the `question` and `passage` fields to our tokenizer and set `truncation=\"only_second\"` to ensure that we only truncate the passages if the question-answer pair exceeds the maximum context length of 512 tokens. The following function does what we need and we can apply it to the whole dataset via the `DatasetDict.map` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_encode(examples): \n",
    "    return tokenizer(examples['question'], examples['passage'], truncation=\"only_second\")\n",
    "\n",
    "boolq_enc = boolq.map(tokenize_and_encode, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Sparse Trainer\n",
    "\n",
    "The next thing to do is create a trainer that can handle the fine-pruning and evaluation steps for us. In `nn_pruning` this is done via the `sparse_trainer.SparseTrainer` [mixin class](https://realpython.com/inheritance-composition-python/#mixing-features-with-mixin-classes) that provides extra methods for `transformers.Trainer` to \"patch\" or sparsify pretrained models and implement the various pruning techniques discussed in the movement pruning paper.\n",
    "\n",
    "To keep things simple, we'll override the `compute_loss` function to ignore knowledge distillation and just return the cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from nn_pruning.sparse_trainer import SparseTrainer\n",
    "\n",
    "class PruningTrainer(SparseTrainer, Trainer):\n",
    "    def __init__(self, sparse_args, *args, **kwargs):\n",
    "        Trainer.__init__(self, *args, **kwargs)\n",
    "        SparseTrainer.__init__(self, sparse_args)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        We override the default loss in SparseTrainer because it throws an \n",
    "        error when run without distillation\n",
    "        \"\"\"\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        self.metrics[\"ce_loss\"] += float(loss)\n",
    "        self.loss_counter += 1\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `SparseTrainer` expects `sparse_args` in its `__init__` method. These arguments are analogous to  `transformers.TrainingArguments` and specify which pruning method is applied, whether knowledge distillation is activated, the associated hyperparameters, and more. Let's take a look at the defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTrainingArguments(mask_scores_learning_rate=0.01, dense_pruning_method='topK', attention_pruning_method='topK', ampere_pruning_method='disabled', attention_output_with_dense=True, bias_mask=True, mask_init='constant', mask_scale=0.0, dense_block_rows=1, dense_block_cols=1, attention_block_rows=1, attention_block_cols=1, initial_threshold=1.0, final_threshold=0.5, initial_warmup=1, final_warmup=2, initial_ampere_temperature=0.0, final_ampere_temperature=20.0, regularization='disabled', regularization_final_lambda=0.0, attention_lambda=1.0, dense_lambda=1.0, distil_teacher_name_or_path=None, distil_alpha_ce=0.5, distil_alpha_teacher=0.5, distil_temperature=2.0, final_finetune=False, layer_norm_patch=False, gelu_patch=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nn_pruning.patch_coordinator import SparseTrainingArguments\n",
    "\n",
    "sparse_args = SparseTrainingArguments()\n",
    "sparse_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main hyperparameters to tweak for fine-pruning are:\n",
    "\n",
    "* `dense_pruning_method` / `attention_pruning_method`: determines how the matrix of mask scores are calculated for the dense/attention layers. Can take one of the following values: \n",
    "    * `l0`: $L_0$ regularization\n",
    "    * `magnitude`: magnitude pruning\n",
    "    * `topK`: Movement pruning\n",
    "    * `sigmoied_threshold`: soft movement pruning\n",
    "* `initial_threshold`: the initial value of the masking threshold for scheduling. Set this to 1 when using `topK` (initial density) or 0 when using `sigmoied_threshold` (cutoff)\n",
    "* `final_threshold`: the final value of the masking threshold. When using `topK`, this is the final density. With `sigmoied_threshold`, a good choice is 0.1 \n",
    "* `initial_warmup`: runs `initial_warmup` * `warmup_steps` steps of threshold warm-up during which threshold stays at its `initial_threshold` value (sparsity schedule)\n",
    "* `final_warmup`: runs `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, let's use `topK` movement pruning and remove 75% of the weights in the encoder. We'll apply a form of \"hybrid pruning\" by performing block pruning on the attention layers and adding the `1d_alt` argument for the dense layers, which prunes alternating rows and columns and produces better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"initial_warmup\": 1,\n",
    "    \"final_warmup\": 3,\n",
    "    \"initial_threshold\": 1.0, \n",
    "    \"final_threshold\": 0.25, \n",
    "    \"dense_pruning_method\": \"topK:1d_alt\", \n",
    "    \"dense_block_rows\":1,\n",
    "    \"dense_block_cols\":1,\n",
    "    \"dense_lambda\":0.25,\n",
    "    \"attention_pruning_method\": \"topK\", \n",
    "    \"attention_block_rows\":32,\n",
    "    \"attention_block_cols\":32,\n",
    "    \"attention_lambda\":1.0,\n",
    "    \"ampere_pruning_method\": \"disabled\",\n",
    "    \"mask_init\": \"constant\",\n",
    "    \"mask_scale\": 0.0,\n",
    "    \"regularization\": None, \n",
    "    \"regularization_final_lambda\": 20, \n",
    "    \"distil_teacher_name_or_path\":None,\n",
    "    \"distil_alpha_ce\": 0.1,\n",
    "    \"distil_alpha_teacher\": 0.9,\n",
    "    \"attention_output_with_dense\": 0,\n",
    "    \"layer_norm_patch\" : 0,\n",
    "    \"gelu_patch\":0\n",
    "}\n",
    "\n",
    "for k,v in hyperparams.items():\n",
    "    if hasattr(sparse_args, k):\n",
    "        setattr(sparse_args, k, v)\n",
    "    else:\n",
    "        print(f\"sparse_args does not have argument {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the pruning hyperparameters, we also need the usual training parameters like learning rate, batch size and so on. These can be configured using `transformers.TrainingArguments` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 8\n",
    "logging_steps = len(boolq_enc[\"train\"]) // batch_size\n",
    "# warmup for 10% of training steps\n",
    "warmup_steps = logging_steps * num_train_epochs * 0.1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    "    disable_tqdm=False,\n",
    "    report_to=None,\n",
    "    save_steps=1e6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: a key ingredient for getting good results with movement pruning is to prune the model slowly by training for several epochs and including some amount of linear warmup (6-10% of the total steps is a good heuristic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching a Dense Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable movement pruning, we need masked versions of BERT-base that can compute the adaptive mask in the forward pass. The way this is done in `nn_pruning` is via the `ModelPatchingCoordinator` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from nn_pruning.patch_coordinator import ModelPatchingCoordinator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mpc = ModelPatchingCoordinator(\n",
    "    sparse_args=sparse_args, \n",
    "    device=device, \n",
    "    cache_dir=\"checkpoints\", \n",
    "    logit_names=\"logits\", \n",
    "    teacher_constructor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class has several methods that control how pruning is applied during training and how to convert a pruned model into a format that is compatible for runnning inference with the `transformers` API. The first thing we need to do is \"patch\" our dense model which can be achieved with the `ModelPatchingCoordinator.patch_model` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER NORM PATCH {'patched': 72}\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_ckpt).to(device)\n",
    "mpc.patch_model(bert_model)\n",
    "\n",
    "bert_model.save_pretrained(\"models/patched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We almost have all the ingredients needed to fine-prune our model! The only thing missing is the `compute_metrics` function for our trainer, so let's load the `accuracy` metric from `datasets` to measure the performance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "accuracy_score = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_score.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to do is instantiate our trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PruningTrainer(\n",
    "    sparse_args=sparse_args,\n",
    "    args=args,\n",
    "    model=bert_model,\n",
    "    train_dataset=boolq_enc[\"train\"],\n",
    "    eval_dataset=boolq_enc[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specify the patch coordinator during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_patch_coordinator(mpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and fine-prune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has been fine-pruned, let's save it for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = \"models/bert-base-uncased-finepruned-boolq\"\n",
    "trainer.save_model(output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise how the dense layers are pruned in alternating rows and columns, while the attention layers are pruned in a blockwise fashion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.11.intermediate.dense.weight tensor(1., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.11.attention.output.dense.weight tensor(1., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "parameters = dict(trainer.model.named_parameters())\n",
    "param_names = [\"bert.encoder.layer.11.intermediate.dense.weight\",\n",
    "               \"bert.encoder.layer.11.attention.output.dense.weight\"]\n",
    "\n",
    "for param_name in param_names:          \n",
    "    w = parameters[param_name]\n",
    "    print(param_name)\n",
    "\n",
    "    plt.imshow((w != 0).detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model has been fine-pruned, the weights that are masked during the forward pass can be set to zero and pruned once for all (which reduces the amount of information to store). This is achieved by applying the `ModelPatchingCoordinator.compile_model` function which will transform the model in-place and make it compatible with `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc.compile_model(trainer.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this alone won't give us any speed-up during inference because matrix multiplication does not get faster just because more values are zero. To take care of this, `nn_pruning` provides an `optimize_model` function that will cleverly remove the zeroes from the model and produce a pruned model that has fewer parameters (and thus faster for inference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed heads 1, total_heads=144, percentage removed=0.006944444444444444\n",
      "bert.encoder.layer.0.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.0.output.dense, sparsity = 84.99\n",
      "bert.encoder.layer.1.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.1.output.dense, sparsity = 84.99\n",
      "bert.encoder.layer.2.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.2.output.dense, sparsity = 84.99\n",
      "bert.encoder.layer.3.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.3.output.dense, sparsity = 84.99\n",
      "bert.encoder.layer.4.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.4.output.dense, sparsity = 84.99\n",
      "bert.encoder.layer.5.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.5.output.dense, sparsity = 84.99\n",
      "bert.encoder.layer.6.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.6.output.dense, sparsity = 84.99\n",
      "bert.encoder.layer.7.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.7.output.dense, sparsity = 84.99\n",
      "bert.encoder.layer.8.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.8.output.dense, sparsity = 84.99\n",
      "bert.encoder.layer.9.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.9.output.dense, sparsity = 84.99\n",
      "bert.encoder.layer.10.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.10.output.dense, sparsity = 84.99\n",
      "bert.encoder.layer.11.intermediate.dense, sparsity = 84.99\n",
      "bert.encoder.layer.11.output.dense, sparsity = 84.99\n"
     ]
    }
   ],
   "source": [
    "from nn_pruning.inference_model_patcher import optimize_model\n",
    "\n",
    "prunebert_model = optimize_model(trainer.model, \"dense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5583447622715394"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prunebert_model.num_parameters() / bert_model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "def compute_latencies(model,\n",
    "                      model_name,\n",
    "                      question=\"Is Saving Private Ryan based on a book?\",\n",
    "                      passage=\"\"\"In 1994, Robert Rodat wrote the script for the film. Rodat’s script was submitted to \n",
    "                      producer Mark Gordon, who liked it and in turn passed it along to Spielberg to direct. The film is \n",
    "                      loosely based on the World War II life stories of the Niland brothers. A shooting date was set for \n",
    "                      June 27, 1997\"\"\"):\n",
    "    inputs = tokenizer(question, passage, truncation=\"only_second\", return_tensors=\"pt\")\n",
    "    latencies = []\n",
    "    \n",
    "    for _ in range(10):\n",
    "        _ = model(**inputs)\n",
    "        \n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ = model(**inputs)\n",
    "        latency = perf_counter() - start_time \n",
    "        latencies.append(latency)\n",
    "        # Compute run statistics\n",
    "        time_avg_ms = 1000 * np.mean(latencies)\n",
    "        time_std_ms = 1000 * np.std(latencies) \n",
    "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\") \n",
    "    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency (ms) - 164.78 +\\- 93.01\n"
     ]
    }
   ],
   "source": [
    "latencies = compute_latencies(prunebert_model.to(\"cpu\"), \"prunebert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ft_model = AutoModelForSequenceClassification.from_pretrained(\"lewtun/bert-base-uncased-finetuned-boolq\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency (ms) - 109.82 +\\- 31.26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time_avg_ms': 109.8177058622241, 'time_std_ms': 31.263346973297544}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latencies.update(compute_latencies(bert_ft_model.to(\"cpu\"), \"bert-base\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpruning",
   "language": "python",
   "name": "nnpruning"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
